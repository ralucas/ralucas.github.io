{"meta":{"title":"Richard Lucas' Blog","subtitle":"Node.js and Networking","description":"Node.js and Networking","author":"Richard Lucas","url":"http://ralucas.github.io"},"pages":[],"posts":[{"title":"Securing Internet Routing","slug":"Securing-Internet-Routing","date":"2016-04-23T06:00:00.000Z","updated":"2016-04-23T16:57:33.000Z","comments":true,"path":"2016/04/23/Securing-Internet-Routing/","link":"","permalink":"http://ralucas.github.io/2016/04/23/Securing-Internet-Routing/","excerpt":"","keywords":null,"text":"Border Gateway Protocol (BGP)BGP, an important piece of the larger Internet’s architecture, is a protocol that enables the setting up of routing and exchange of reachability information between large networks (Autonomous Systems). It’s the protocol used to develop routes between these Autonomous Systems (ASes), routing to destinations via IP prefixes. BGP makes announcements that the ASes use to discover routes to these IP prefixes. BGP is important to the business of the Internet, advertising routes, routing based on business relationships. ASes usually make sure that it’s in their “best” interest before routing traffic and there is a rule-of-thumb that seems to be generally followed: An AS will advertise a route to a neighboring AS if: The neighbor is a customer, or The route is for a prefix originated by the advertising AS, or The route is through a customer of the advertising AS BGP was originally introduced, like many of the protocols in use in the Internet, in the early days, when the Internet was more innocent. So, issues with security persist surrounding the protocol. BGP Security RisksHijacksBGP doesn’t have in place any mechanism to properly authenticate allocations of IP prefixes to ASes, so as a result entire IP prefix blocks can be hijacked fairly easily, either accidently or purposefully. There are two main types of hijacks: Prefix and subprefix. Prefix:The hijacker AS originates and announces the exact same prefix as a legitimate AS that has the IP allocation. The announcement gets propogated through the system and other ASes begin chosing their routes for that IP prefix and some will chose to go through the bogus AS, others won’t, given route length. Subprefix:The hijacker AS can potentially intercept 100% of the network traffic. Here the hijacker originates a prefix that is covered by the victim IP prefix. BGP uses longest-prefix match (LPM), so if a hijacker were to advertise a prefix that was LPM, then traffic would redirect to the hijacker AS. Route LeaksThis isn’t a bogus route, but instead the leaker announces a legitimate route, but does it to too many of its neighbors. So, then the leaker is overwhelmed by traffic from its neighbors that are now utilizing the leaked route. This can be disastrous if the leaker is not designed to handle high levels of traffic. Path-Shortening AttackAn attacker announces a short bogus path to a prefix that terminates at an authorized origin AS. Protocol downgrade attackASes that have, for instance, deployed BGPSEC can be convinced by an attacker to select a bogus path instead of the secured route because it’s cheaper. Incident ImpactBlackhole:Network traffic stops at the perpetrator AS and goes no further. Interception:Perpetrator AS invisibly intercepts traffic and the traffic also continues onto it’s intended destination. BGP Defense MechanismsPrefix filteringA whitelisting technique used to filter out bad BGP announcements. It works by using the AS rule-of-thumb and keeping a prefix list of IP prefixes of customers and ignoring any announcement from a customer not on the list. This defense has been used since the 1990s. Upsides: It is simple and effective mechanism and if all ASes deployed it, a large portion of routing leaks and hijacks would be prevented. Downsides: The downside is that it only works on customer leaks. ASes also aren’t necessarily incentivized to deploy this filtering outside of good Internet citzenship. RPKIResource Public Key Infrastructure (RPKI) is another defense method that provides a trusted mapping from allocated IP prefixes to BGP authorized ASes for origination. It creates a cryptographic hierarchy of authorities, which is rooted at the regional Internet registries (i.e. ARIN, RIPE, AfriNIC, etc.). The holder of the certificate for a prefix can then sign an authorization allowing a prefix to be originated via BGP. Upsides: Does not require any modification to current BGP message formats Cryptography can be performed offline As opposed to Prefix Filtering, it doesn’t tie itself to potential political/business conflict of interests as it can be used to filter BGP announcements made by any neighbor. Downsides: Potential for abuse of the RPKI, i.e. RPKI being attacked, misconfigured, or abused in some other way in which trust is lost in the protocol. Cannot prevent route leaking attacks as it’s purpose is to prevent unauthorized messages, whereas route leaks come from authorized origins. Cannot prevent path-shortening attack as the origin is legitimate and shortest path takes precedence. BGPSECBGPSEC, currently in standardization process by the IETF, builds on RPKI, adding crypto signatures to BCP messages. Each AS must sign a BGP message upon announcement. The signature includes the prefix and AS-level path, the AS number of the AS receiving the message, and all the previous signed messages received from previous ASes on the path. Upsides: No path-shortening attacks are possible because a shortened path would not pass the signature checks required as the origin would be checked against neighbor signatures. Downsides: Unlike RPKI, BGPSEC is online crypto as routers sign and verify the BGP messages. This has a higher computational load, requiring the routers to be designed and built with that in mind. To gain the full benefits of BGPSEC, every AS needs to deploy it. This requires that the already decentralized ASes, who have their own political and business objectives, to agree to use this protocol. To remedy this, one way is to gain traction via early adoption by some of the ASes ASes tend to prioritize economic demands over those of security demands and given that BGPSEC only provides some small benefits over RPKI, ASes are not as incentivized. Suffers from protocol downgrade attacks. [1] Goldberg, 2014. Why is it taking so long to secure internet routing","raw":null,"content":null,"categories":[],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"BGP","slug":"BGP","permalink":"http://ralucas.github.io/tags/BGP/"},{"name":"Security","slug":"Security","permalink":"http://ralucas.github.io/tags/Security/"},{"name":"Routing","slug":"Routing","permalink":"http://ralucas.github.io/tags/Routing/"}]},{"title":"Jellyfish Data Center Topology Review","slug":"Jellyfish-Data-Center-Topology-Review","date":"2016-04-22T06:00:00.000Z","updated":"2016-04-23T01:35:02.000Z","comments":true,"path":"2016/04/22/Jellyfish-Data-Center-Topology-Review/","link":"","permalink":"http://ralucas.github.io/2016/04/22/Jellyfish-Data-Center-Topology-Review/","excerpt":"","keywords":null,"text":"What is Jellyfish?Jellyfish, an incrementally-expandable, high-bandwidth data center networking topology based on randomness. Definition: a degree-bounded random graph topology among top-of-rack switches. Basically, it’s an unstructured network that utilizes a randomized topology for interconnection in a data center, unlike, for example a fat tree topology that utilizes hierarchies (Figure 1). It allows for construction of arbitrary-size networks that can be easily incrementally expanded as needs arise in the data center for capacity or bandwidth. Jellyfish challengesAs a result of an unstructured data center network, challenges arise: Routing Physical construction Cabling layout How it’s constructedUsing the assumption that every switch has the same number of ports and servers, then with N racks, the network supports N(k - r) servers, where k is the number of ports and r is the number of ports used to connect to other racks. So, k - r is the number of ports that can be used to connect to servers. To wire this up, just pick a random pair of switches with free ports and link them and continue to do this process until all links are exhausted. Key: Path LengthAccording to the authors, the end-to-end throughput of a data center’s topology (how fast does data flow through it) isn’t solely dependent on the capacity (bandwidth/speed) of the network. Another important metric is the amount of network capacity that’s consumed in delivering each byte. This is represented in average path length. In order to deliver each byte of data, a network takes a series of hops in delivering that data. The less hops, the lower the average path length, the less network capacity consumed, therefore the faster the data can flow. Jellyfish has a lower average path length than standard hierarchal models. (a) Fat-tree path lengths (b) Jellyfish path lengths Key: Flexibility and ExpansionJellyfish’s construction is such that incrementally adding even just one server rack or switch is quite simple. The only rewiring necessary is limited to the number of ports being added. Additionally, Jellyfish allows for expansion using newer equipment that may have higher port-counts. One note here: Expansion may not produce uniform-random graph. However, topologies built incrementally versus those built from scratch show similar capacity throughputs and path lengths. Key: ResilienceJellyfish, in the face of failures, still maintains it’s structure. Additionally, Jellyfish, in comparison with a fat-tree topology with less servers, supports the same capacity, path-length, and resilience. CablingIn Jellyfish, there are more than twice as many cables running between switches than from servers to switches, so placing all the switches closely together makes sense. It’s suggested that the switch-cluster, where the majority of cables congregate, be placed in the center of the data center, with aggregate cable bundles running to each server-rack. Space should be left in the middle for additional switches and then more servers will just be added to the outside. In massive data centers, Jellyfish can be adapted with all of the pluses that it offers over fat-tree. ConclusionJellyfish, a random graph based network topology, is a scalable topology for data centers that makes setup and expandability simple. Jellyfish, also, can support 25% more servers than fat-tree, has on-average shorter path-length, is highly failure resilient, and shows network capacity of better than 90% of other known topologies.","raw":null,"content":null,"categories":[],"tags":[{"name":"Jellyfish","slug":"Jellyfish","permalink":"http://ralucas.github.io/tags/Jellyfish/"},{"name":"Data centers","slug":"Data-centers","permalink":"http://ralucas.github.io/tags/Data-centers/"},{"name":"Scaling","slug":"Scaling","permalink":"http://ralucas.github.io/tags/Scaling/"},{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"}]},{"title":"Understanding DASH (Dynamic Adaptive Streaming over HTTP)","slug":"Understanding-Dynamic-Adaptive-Streaming-over-HTTP","date":"2016-04-20T06:00:00.000Z","updated":"2016-04-20T07:08:08.000Z","comments":true,"path":"2016/04/20/Understanding-Dynamic-Adaptive-Streaming-over-HTTP/","link":"","permalink":"http://ralucas.github.io/2016/04/20/Understanding-Dynamic-Adaptive-Streaming-over-HTTP/","excerpt":"","keywords":null,"text":"What is DASH, comparatively?Dynamic Adaptive Streaming over HTTP (DASH) is a standard that intends to address weaknesses in RTSP and progressive download methods currently used in streaming content over the internet. RTSPRTSP is a streaming protocol that manages a stateful connection with the client, tracking the state through the client interation. ProgressiveProgressive download can utilize HTTP and uses byte range requests to respond to the client. The issues with progressive are that it’s not bitrate responsive, bandwidth can potentially be wasted if the user switches their request, and doesn’t do live media. Despite these weakeness, progressive download enjoys wide adoption Why use HTTP?Content Delivery Networks (CDNs) have proliferated all over the world and given this, their scale, and their reliability, using them to manage streaming services just makes sense and…they use HTTP. Here are the nine reason’s Mr. Stockhammer gives [1] (my paraphrasing): HTTP Streaming is spreading widely as a form of delivery of Internet Video Trend towards HTTP as the main protocol for media delivery over the Internet Given the wide deployment of HTTP along with the TCP/IP stack, it probides reliabiliy and easy deployment HTTP avoids NAT and firewall traversal issues as it doesn’t require a dedicated connection…it’s stateless Allows delivery mechanisms and common methods such as CDNs, HTTP Caches, and HTTP Servers to be used, which already exist in profligate Moves control of the HTTP session to the client Provides the ability of the client to choose content rate given bandwidth as opposed to having the streaming server negotiate that Allows content rate to be seamlessly changed on the fly given changes in bandwidth Has the potential to accelerate fixed-mobile convergence. DASH GoalsThe DASH solution is intended to: support delivery of media in ISO base media file formats stay out of presentation logic permit integration of different presentation frameworks The protocol then in turn intends to define: Media Presentation as a stuctured collection of data Formats of Segments, an integral data unit of a Media Presentation, that can be HTTP URI addressable Delivery protocol of Segment, i.e. HTTP/1.1 A how-to for the client to utitlize the above information to establish a streaming service It supports features: Fast initial startup and seeking bandwidth efficiency adaptive bitrate switching adaption to CDN properties reuse of HTTP-servers and caches reuse of existing media playout engines Support for on-demand, live play Simplicity OverallDASH is a media streaming protocol that seeks to improve upon some of the known issues with RTSP and progressive download, while taking advantage of an already common and well-supported application protocol, HTTP. It was intended to be easily adapted to existing infrastructure, particularly the usage of HTTP CDNs. It’s intended to be generic enough so that it’s independent of media format. It’s main features are it’s adaptatability to changing network bandwidth, existing HTTP infrastructure, media formats, and flexibility to client presentations. [1] Stockhammer, 2011. Dynamic Adaptive Streaming over HTTP - Design Principles and Standards[2] Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update, 2015–2020 White Paper, February 2016","raw":null,"content":null,"categories":[],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"HTTP","slug":"HTTP","permalink":"http://ralucas.github.io/tags/HTTP/"},{"name":"DASH","slug":"DASH","permalink":"http://ralucas.github.io/tags/DASH/"},{"name":"Streaming","slug":"Streaming","permalink":"http://ralucas.github.io/tags/Streaming/"}]},{"title":"Understanding Controlling Queue Delay and CoDel","slug":"Understanding-Controlling-Queue-Delay","date":"2016-04-19T06:00:00.000Z","updated":"2016-04-20T04:25:28.000Z","comments":true,"path":"2016/04/19/Understanding-Controlling-Queue-Delay/","link":"","permalink":"http://ralucas.github.io/2016/04/19/Understanding-Controlling-Queue-Delay/","excerpt":"","keywords":null,"text":"This is a survey of the “Controlling Queue Delaty” academic paper by Kathleen Nichols and Van Jacobson from ACMQueue in 2012.[1] Bufferbloat is still a problem!Packet-based networks use buffers to handle short-term fluctuation in traffic flow. Bufferbloat describes the standing packet queue that’s created when there is a mismatch between the senders maximum send rate (window size) and the bottleneck’s pipe size. This issue of bufferbloat has been known for many decades now. And given all the streaming today, it’s an important issue as applications today are much more delay-sensitive. The Standing QueueThis standing queue is the result from this mismatch. Because it creates delays and doesn’t improve throughput, it gets seen as a congestion issue and not properly seen in the light of queue or traffic theory. This is due to theorists seeing TCP’s reliable and highly correlated process, in which the packet arrival rate equals the departure rate, as improbable. Not an easy problemAnd the problem has been difficult to solve. Given that senders set the window size, queues begin to build up at the bottlenecks. This, thereby, makes it difficult for senders to adjust their sending rate given the constantly changing with conditions (flows coming and going, physical conditions, etc.) that occur in the bandwidth of the bottlenecks. Reliable Detection is HardA large queue is necessary to get started as the buffer needs to initially fill, so queue size really doesn’t give us much information about excess queue. A good queue is occupancy that goes away in about one RTT; bad queue persists for several RTTs [1] Active Queue Management (AQM)Active Queue Management is the solution for this bufferbloat problem, but it’s lack of widespread adoption has emanated from implementation difficulties and misunderstanding in the overall issue. AQM Goals Paramaterless - no implementation needed Treats good and bad queue differently Controls delay Adapts to dynamically changing link rates Simple and efficient - from home to enterprise use CoDel: A proposed solutionCoDel (Contolled Delay Management) is a proposed piece to that solution to this issue of queue management. It makes three main innovations to it’s algorithm: Using the local minimum queue to measure standing queue and not based on any of the previously used metrics such as the average size, etc. Keeping a single-state variable measuring how long the minimum have above or below the target for standing queue rather than a window of values to compute the minimum. Using the packet-sojourn time through the queue as the unit of measure as opposed to bytes or packets. Why the minimum value?The minimum packet sojourn can be decreased only when a packet is dequeued, so CoDel can go to work when packets are dequeued for sending and no locks are needed in the software. If the buffer is full on arrival, then a packet is dropped. The target and intervalThe target represents the “acceptable standing queue delay.” Very importantly, it’s measured in packet-sojourn time, not bytes or packets. Additionally, it’s unacceptable to drop packets if there are fewer than one MTU of bytes in the buffer.The interval represents “a time on the order of a worst-case RTT of connections through the bottleneck.” Let’s put it togetherSo, the crux here is that when the queue delay (it’s packet-sojourn time) is more than the target for at least interval time, then a packet is dropped and it’s not until the queue delay gets itself back below target that it stops dropping packets. In it’s dropping mode, it’s ‘next drop time’ is decreased each time as 1/SqRt(total_number_of_drops) with the intention of getting a linear change in throughput (TCP flow rate). ConclusionCoDel looks to be the best way to attack queue delay. While not the silver bullet for queue delay, it satisfies the five goals it set out. Manufacturers still need to build and market devices with proper buffer management. Further ReadingCoDel IETF DraftCoDel Wiki [1] Nichols and Jacobsen, 2012. Controlling Queue Delay","raw":null,"content":null,"categories":[],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"TCP","slug":"TCP","permalink":"http://ralucas.github.io/tags/TCP/"},{"name":"Router buffers","slug":"Router-buffers","permalink":"http://ralucas.github.io/tags/Router-buffers/"},{"name":"Bufferbloat","slug":"Bufferbloat","permalink":"http://ralucas.github.io/tags/Bufferbloat/"},{"name":"CoDel","slug":"CoDel","permalink":"http://ralucas.github.io/tags/CoDel/"},{"name":"Queue delay","slug":"Queue-delay","permalink":"http://ralucas.github.io/tags/Queue-delay/"}]},{"title":"Understanding Sizing Router Buffers","slug":"Understanding-Sizing-Router-Buffers","date":"2016-04-17T06:00:00.000Z","updated":"2016-04-20T04:22:36.000Z","comments":true,"path":"2016/04/17/Understanding-Sizing-Router-Buffers/","link":"","permalink":"http://ralucas.github.io/2016/04/17/Understanding-Sizing-Router-Buffers/","excerpt":"","keywords":null,"text":"This is a survey of the “Sizing Routing Buffers” academic paper by Appenzeller, Keslassy, and McKeown published in 2004. In the past, Internet routers had buffers that used a rule-of-thumb using the equation of: B = RTT X C. Where RTT is the round-trip time of a packet and C is the data rate. This rule-of-thumb is no longer useful. The primary goal of the rule-of-thumb, or any for that matter, is to keep a link as close to 100% utilization as possible, as this maximizes the throughput of the network. Why is it no longer useful? Large buffers are difficult to manufacture, given that they must use slow, off-chip DRAM for memory buffering. DRAM (dynamic RAM) has some problems. It has an access time of about 50ns, whereas a packet of size 40bytes, can arrive and depart in 8ns: DRAM is slow and slows network speed down. Additionally, they require utilization of a wide DRAM bus, which requires a large number of data pins, which in turn increases the power consumption of the DRAM boards. Large buffers can contribute to lengthening queueing delays which can in turn cause problems with congestion control algorithms, as they rely on packet loss in order to inform them of TCP congestion. The original rule-of-thumb came from a 1994 paper and experiments that used a small number of multiplexed flows (up to 8) on a now comparatively low-speed connection of only 40 Mb/s. Today, for example, typical backbone links are now beginning to operate at 100 Gb/s or greater[2] with many thousand multiplexed flows. Some consider router buffers to be one of the largest contributors to network traffic speed uncertainty. Why is Overbuffering bad? In order to accomodate large buffers, routers have to be designed accordingly, leading to larger power consumption, more board space, and lower density. It increases delay when congestion occurs and conflict with low-latency real-time applications like video games. And in some cases, make applications completely unusable. One of the main issues with the idea that a buffer should be utilized to maximize the network throughput is that it doesn’t take into account TCP Congestion Control algorithms that rely on a packet loss to indicate congestion and to go into action. TCP will always make the buffer overflow in order to lead to packet loss. Single-flow and Synchronized flowsAccording to the paper, “the key to sizing the buffer is to make sure that the buffer is large enough, so that while the sender pauses, the buffer doesn’t go empty.” In the case of the single, long-lived flow and synchronized TCP flows, for the buffer to never go empty it needs to be half of the window size max. This is because of how TCP performs. When it TCP sees packet loss, it goes into AIMD congestion control mode and drops it’s packet flow by half. So, while the buffer almost hits zero, it never quite does. Desynchronized flowsMost internet backbones handle enough simultaneous flows, i.e. greater than 500, that they usually operate under a desynchronized flow pattern. In this environment, the TCP sawtooth isn’t synched, so the more flows that are added, the window size from peak to trough is smoothed out and gets smaller. So, given that as discussed before, that the amount of buffer needed to maintain the network utilization was half of the window max size, then if the window size is now smaller, then therefore the buffer size can be even smaller. Also, in this case, the flow length distribution is heavily distributed towards long flows, as they typically take up 80% of the bandwidth. However, most of the flows are in fact from short-lived flows (fewer than 100 packets). But, they only account for about 20% of the bandwidth. This is partly due because short flows usually don’t get out of TCP slow-start mode and never fully utilize the network bandwidth, whereas long flows typically get into congestion control mode, maximizing the network bandwidth. ConclusionsThrough this survey, it is apparent that smaller buffer sizes are imperative for best network bandwidth utilization no matter the type of flow or flow environment. Smaller buffers increase bandwidth utilization because they 1) signal to the TCP flow when to go into congestion mode sooner, 2) larger buffers can lead to delay, and 3) are cheaper to manufacture. [1] Appenzaller, Keslassy, McKeown, 2004. Sizing Routing Buffers[2] Malik, 2013. 100G, 200G, 400G: Internet’s core is getting fatter to meet our tech planet’s bandwidth demand","raw":null,"content":null,"categories":[],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"TCP","slug":"TCP","permalink":"http://ralucas.github.io/tags/TCP/"},{"name":"Router buffers","slug":"Router-buffers","permalink":"http://ralucas.github.io/tags/Router-buffers/"}]},{"title":"Useful CLI commands","slug":"Useful-CLI-Commands","date":"2015-08-15T03:37:59.000Z","updated":"2015-08-15T03:37:59.000Z","comments":true,"path":"2015/08/14/Useful-CLI-Commands/","link":"","permalink":"http://ralucas.github.io/2015/08/14/Useful-CLI-Commands/","excerpt":"","keywords":null,"text":"###Some CLI Commands that I’ve found useful: ####Viewing processes and killing them $ lsof -Pi | grep LISTEN Displays processes running and ports The PID is the second number (2nd column from left) $ kill -9 $PID This kills the process at the entered PID with extreme prejudice ####Using find $ find . -name &#39;*.css&#39; This will recurse all directories and list all CSS files (and directories ending with “.css”) under the current directory (represented by “.”) $ find . -type f -name &#39;*.css&#39; This will only match CSS files (case-sensitively) $ find . -name &quot;*.css&quot; -exec grep -l &quot;#content&quot; {} \\; This example finds all CSS files that do something with your HTML ID #content $ find . -mtime -1 -type f find files changed in the last 1 day $ find . \\! -path &quot;*CVS*&quot; -type f -name &quot;*.css&quot; find CSS files, omitting results containing “CVS” $ find ~/src -newer main.css find files newer than main.css in ~/src $ find . -name \\*.css -print0 | xargs -0 grep -nH foo combine with xargs for more power than -exec ####Deleting a folder and it’s contentsPlease use carefully $ rm -rf [folder_name] Always include a folder name ####Copying a folder and contents $ cp -avr /from/folder /to/new_folder -a : Preserve the specified attributes such as directory an file mode, ownership, timestamps, if possible additional attributes: context, links, xattr, all. -v : Explain what is being done (verbose). -r : Copy directories recursively. ####What is grep It is a command that finds text/strings inside of files It can take regular expressions ####Make a file (…for testing…) mkfile command input number, then size (i.e. k for kb or g for gb), then filename $ mkfile -n [size][b|k|m|g] [filename] Heroku commands Get to the console/bash on the remote server $ heroku run bash --app [put_app_name_here] Compiling bash scripts$ chmod +x ./[bash_script.sh] Redirect to port 80 on linux server$ sudo iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 3000","raw":null,"content":null,"categories":[],"tags":[{"name":"CLI","slug":"CLI","permalink":"http://ralucas.github.io/tags/CLI/"},{"name":"Linux","slug":"Linux","permalink":"http://ralucas.github.io/tags/Linux/"}]},{"title":"Resizing image with imagemagick","slug":"Resizing-image-with-imagemagick","date":"2015-08-12T03:46:31.000Z","updated":"2015-08-12T03:46:31.000Z","comments":true,"path":"2015/08/11/Resizing-image-with-imagemagick/","link":"","permalink":"http://ralucas.github.io/2015/08/11/Resizing-image-with-imagemagick/","excerpt":"","keywords":null,"text":"Quick one-linerUsing imagemagick to resize an image, keep it’s proportion (i.e. 4 x 3 ), and place it in the middle with a white background: 1convert input.png -resize 1280x800 -background white -gravity center -extent 1280x800 output.png","raw":null,"content":null,"categories":[],"tags":[{"name":"imagemagick","slug":"imagemagick","permalink":"http://ralucas.github.io/tags/imagemagick/"},{"name":"resizing images","slug":"resizing-images","permalink":"http://ralucas.github.io/tags/resizing-images/"}]},{"title":"Mocking out the Request module in Node.js for Testing","slug":"Mocking-out-the-request-module-for-testing","date":"2015-08-11T05:09:26.000Z","updated":"2015-08-11T05:09:26.000Z","comments":true,"path":"2015/08/10/Mocking-out-the-request-module-for-testing/","link":"","permalink":"http://ralucas.github.io/2015/08/10/Mocking-out-the-request-module-for-testing/","excerpt":"","keywords":null,"text":"So, recently I needed to write a test that required that I mock the request module out as I wanted to pass a specific error code back to the response handler and do something different with it. In this case, I wanted to retry the request. I first attacked the issue with Sinon.js library. If you’ve ever tested in javascript, sinon.js is an indispensable mocking and spying library. So, I wrote: 1234567891011...var request = require(&apos;request&apos;),...before(done) &#123; stub = sinon.stub(request) .withArgs(&#123; method: &apos;GET&apos;, url: &apos;http://example.com&apos; &#125;); done();&#125; However, that doesn’t work as sinon.js requires if you’re stubbing out a module for there to be method that you use, i.e. stub = sinon.stub(request, &#39;get&#39;) … see this StackOverflow issue The problem was that the api that I was testing had already largely been written and I didn’t now want to rewrite a bunch of it just to make it testable for this one case. So, enter Mockery. Mockery is great in this instance, but be careful when you use it as it tears down the require statement, so use it as tightly as you can and tear it down immediately after use. So, I separated out my test: myModule.js: 123456789101112131415161718...var request = require(&apos;request&apos;);...function handler(callback) &#123; return function(err, res, body) &#123; if (err) &#123; //doStuff &#125; ...//doOtherStuff &#125;&#125;exports.getSomeStuff = function(url, callback) &#123; return request(&#123; method: &apos;GET&apos;, url: url &#125;, handler(callback));&#125;; test.js: 1234567891011121314151617181920212223242526272829303132333435363738394041424344...var request = require(&apos;request&apos;);... describe(&apos;request error testing&apos;, function() &#123; var stub, module; before(function(done) &#123; mockery.enable(&#123; warnOnReplace: false, warnOnUnregistered: false, useCleanCache: true &#125;); stub = sinon.stub(); mockery.registerMock(&apos;request&apos;, stub); mm = require(&apos;./myModule&apos;); done(); &#125;); after(function(done) &#123; mockery.disable(); done(); &#125;); it(&apos;should retry once on ETIMEDOUT error&apos;, function(done) &#123; var spy = sinon.spy(client, &apos;request&apos;); var err = new Error(&apos;connect ETIMEDOUT&apos;); stub.yields(err, null, null); mm.getSomeStuff(&apos;http://example.com&apos;, function(err, json) &#123; expect(spy.calledOnce).to.be.true; expect(stub.calledTwice).to.be.true; expect(err.message).to.include(&apos;ETIMEDOUT&apos;); done(); &#125;); &#125;); &#125;); So, there you have it. Comment if you have any thoughts or questions. Another really great post that helped me: http://bulkan-evcimen.com/using_mockery_to_mock_modules_nodejs.html Cheers! Richard","raw":null,"content":null,"categories":[],"tags":[{"name":"Testing","slug":"Testing","permalink":"http://ralucas.github.io/tags/Testing/"},{"name":"Node.js","slug":"Node-js","permalink":"http://ralucas.github.io/tags/Node-js/"},{"name":"Request","slug":"Request","permalink":"http://ralucas.github.io/tags/Request/"},{"name":"Mockery","slug":"Mockery","permalink":"http://ralucas.github.io/tags/Mockery/"},{"name":"Sinon.js","slug":"Sinon-js","permalink":"http://ralucas.github.io/tags/Sinon-js/"}]},{"title":"Hello","slug":"Hello","date":"2014-12-24T01:36:49.000Z","updated":"2014-12-24T01:45:26.000Z","comments":true,"path":"2014/12/23/Hello/","link":"","permalink":"http://ralucas.github.io/2014/12/23/Hello/","excerpt":"","keywords":null,"text":"Welcome to my Github Blog and Projects Blog. I focus mostly on Node.js and Javascript. Please feel free to message me anytime through my profile @ralucas, via twitter @r_a_lucas, or email richard@richardalucas.com. Cheers! Richard Current Projects: Node-Zillow","raw":null,"content":null,"categories":[],"tags":[]},{"title":"Importing CSV in Postgres","slug":"Importing-CSV-in-Postgres","date":"2013-12-11T01:19:18.000Z","updated":"2015-02-02T01:19:50.000Z","comments":true,"path":"2013/12/10/Importing-CSV-in-Postgres/","link":"","permalink":"http://ralucas.github.io/2013/12/10/Importing-CSV-in-Postgres/","excerpt":"","keywords":null,"text":"Log into the database from the terminal:1$ psql database_name Create a new table:123456789# CREATE TABLE table_name(column_name data_type, column_name2 data_type, etc);example:# CREATE TABLE trips(block_id int,route_id text,direction_id int,trip_headsign text,shape_id text,service_id text,trip_id text);//And then you should see:CREATE TABLE Copy the data from a CSV:12345678# COPY table_name FROM &apos;/path/to/CSV_file.txt&apos; DELIMITER &apos;,&apos; CSV;example:# COPY trips FROM &apos;/home/username/Downloads/data/trips.txt&apos; DELIMITER &apos;,&apos; CSV;//And then you should see:COPY XXXX &lt;-number of lines View the tables and select the table you wish to view:12# \\dt# SELECT * FROM table_name; Cheers!","raw":null,"content":null,"categories":[],"tags":[{"name":"Postgres","slug":"Postgres","permalink":"http://ralucas.github.io/tags/Postgres/"},{"name":"CSV","slug":"CSV","permalink":"http://ralucas.github.io/tags/CSV/"}]},{"title":"Installing Postgres on Linux Mint","slug":"Installing-Postgres-on-Linux-Mint","date":"2013-12-02T01:10:59.000Z","updated":"2015-02-02T03:25:55.000Z","comments":true,"path":"2013/12/01/Installing-Postgres-on-Linux-Mint/","link":"","permalink":"http://ralucas.github.io/2013/12/01/Installing-Postgres-on-Linux-Mint/","excerpt":"","keywords":null,"text":"###A Quick Guide Open up the terminal:1$ sudo apt-get install postgresql Installs Postgresql. This will take a few minutes. Next:1$ sudo su - postgres This logs you into the database as the postgres user. You will then want to create yourself as a user:1$ createuser your_username Now, at the command prompt under your username, you are able to use createdb and other commands. So, let’s create a quick database and connect to it.1$ createdb myFirstDb Connect to it:1$ psql myFirstDb You will then see a prompt that will look like:1myFirstDb=# The # indicates that you are the superuser, which is most likely if you just installed postgres on your local computer. Let’s run:1myFirstDb=# SELECT Version(); This should go into a new screen on your terminal that should look something like this:123456 version ------------------------------------------------------------------------------------------------------- PostgreSQL 9.1.10 on i686-pc-linux-gnu, compiled by gcc (Ubuntu/Linaro 4.8.1-10ubuntu7) 4.8.1, 32-bit(1 row)(END) Type q to exit the screen and go back to the # prompt. You can run commands from the # prompt using \\ notation and the official documentation on commands can be found here: http://www.postgresql.org/docs/9.3/interactive/app-psql.html If you want a GUI client:1$ sudo apt-get install pgadmin3","raw":null,"content":null,"categories":[],"tags":[{"name":"Postgres","slug":"Postgres","permalink":"http://ralucas.github.io/tags/Postgres/"},{"name":"Linux Mint","slug":"Linux-Mint","permalink":"http://ralucas.github.io/tags/Linux-Mint/"}]},{"title":"Using Crockford's supplant function in Javascript","slug":"Using-Crockford-s-supplant-function-in-Javascript","date":"2013-11-02T00:09:07.000Z","updated":"2015-02-02T01:10:14.000Z","comments":true,"path":"2013/11/01/Using-Crockford-s-supplant-function-in-Javascript/","link":"","permalink":"http://ralucas.github.io/2013/11/01/Using-Crockford-s-supplant-function-in-Javascript/","excerpt":"","keywords":null,"text":"The supplant function is easy to use for string interpolation and a nice introduction templating in JavaScript. Here’s the function from Douglas Crockford’s website: 123456789if (!String.prototype.supplant) &#123; String.prototype.supplant = function (o) &#123; return this.replace(/\\&#123;([^&#123;&#125;]*)\\&#125;/g, function (a, b) &#123; var r = o[b]; return typeof r === &apos;string&apos; || typeof r === &apos;number&apos; ? r : a; &#125;); &#125;;&#125; So how is it used. Here’s a quick example: var greeting = &quot;Hi, I&#39;m {name}&quot;.supplant[{name:&quot;Richard&quot;}]; Output: “Hi, I’m Richard” Another example: 1234567myObj = &#123; name: &apos;Richard&apos;, city: &apos;Columbus&apos;, state: &apos;Ohio&apos; &#125;; var myInfo = &quot;Hi, my name is &#123;name&#125; and I&apos;m from &#123;city&#125;, &#123;state&#125;&quot;.supplant(myObj); Output: “Hi, my name is Richard and I’m from Columbus, Ohio” You can use it with arrays: 123var names = [&quot;Richard&quot;, &quot;Yalcin&quot;, &quot;Dan&quot;, &quot;Mike&quot;, &quot;Kerry&quot;];var classmates = &quot;My classmates are &#123;0&#125;, &#123;1&#125;, &#123;2&#125;, &#123;3&#125;, and &#123;4&#125;&quot;.supplant(names); Output: “My classmates are Richard, Yalcin, Dan, Mike, and Kerry” These are a few examples, hopefully you do find them helpful. Supplant does take some criticism for being inefficient as well as unescapable, which are both founded. So, you may want to use supplant sparingly, but it’s certainly a great tool in learning JavaScript. Here’s a stack overflow post that discusses it further as well. And as you delve deeper into string interpolation and templating, I suggest looking into Handlebars.js or Mustache. As always, if you have any questions, please don’t hesitate to ask. Cheers!","raw":null,"content":null,"categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://ralucas.github.io/tags/JavaScript/"},{"name":"Supplant","slug":"Supplant","permalink":"http://ralucas.github.io/tags/Supplant/"},{"name":"Templating","slug":"Templating","permalink":"http://ralucas.github.io/tags/Templating/"}]},{"title":"Late Fragment by Raymond Carver","slug":"Late-Fragment-by-Raymond-Carver","date":"2013-07-02T00:05:36.000Z","updated":"2015-02-02T01:43:40.000Z","comments":true,"path":"2013/07/01/Late-Fragment-by-Raymond-Carver/","link":"","permalink":"http://ralucas.github.io/2013/07/01/Late-Fragment-by-Raymond-Carver/","excerpt":"","keywords":null,"text":"Great poem by one of my favorite writers. Late Fragmentby Raymond Carver And did you get whatyou wanted from this life, even so?I did.And what did you want?To call myself beloved, to feel myselfbeloved on the earth.","raw":null,"content":null,"categories":[],"tags":[{"name":"Raymond Carver","slug":"Raymond-Carver","permalink":"http://ralucas.github.io/tags/Raymond-Carver/"},{"name":"Poem","slug":"Poem","permalink":"http://ralucas.github.io/tags/Poem/"}]},{"title":"Cleveland Marathon Race Report","slug":"Cleveland-Marathon-Race-Report","date":"2013-06-02T00:06:57.000Z","updated":"2015-02-02T03:25:42.000Z","comments":true,"path":"2013/06/01/Cleveland-Marathon-Race-Report/","link":"","permalink":"http://ralucas.github.io/2013/06/01/Cleveland-Marathon-Race-Report/","excerpt":"","keywords":null,"text":"This past Sunday, I ran in the Cleveland Marathon. I’d been training for the past 4-6 months for the race. So, how did it go? Well, I finished with a 3:43 time, so reasonably well. I beat my goal time of 3:45 by a couple of minutes, so overall I’m quite happy with the results. As I’ve mentioned earlier, I’ve been quite worried that my right foot was going to bother me the entire race, but after about 2 miles or so, it went away or I completely forgot about it. ####My race plan: Start out a little slow at a 9-minute pace for the first few miles and then begin to speed up. Try to speed up in the second half of the race for a negative split. Try to speed up after mile 16, in which the majority of the course would be heading down. For the most part I stuck to this plan. I ran easy in the beginning and sped up a little, hitting the half marathon point at 1:50. I then actually did speed up for the next split at 30K, doing a 8:17 split. I was on pace and I was feeling great. We were running through Rockefeller park, underneath the shade, and it was perfect. But, then I started to slow up, particularly around mile 22. This seemed to be the loneliest, hottest stretch of the race. It was around I-90, there weren’t too many folks around, no shade and it was a long uphill. That was a battle. Between miles 21 and 25, I was really struggling. My heart was working hard and I was really starting to feel it. At the mile 23 water stop, I actually did stop to walk through it taking both a Powerade and water. I walked for a minute or so, but knew I needed to get going. Those couple of miles between 23 and 25, as I could see the downtown in the distance were tough miles. I tried to gut it out, keeping a 9:15 pace going. When I reached one mile to go, I tried to give it all I had and really got going up to speed with about a half mile to go. And I made it with a good time. ####My nutrition plan: Clif Shot Blocks - eat one every two miles just before the water station and get some water to wash it down. Then switch to the Honey Stingers - I broke these up and put them in a little ziplock bag. I stuck to the plan. I took one package (6 pieces) of the Shot Blocks, so for the first twelve miles I did that and then switched over to the Honey Stingers for the second half. I think it worked for the most part as I didn’t have any noticeable sugar fall-outs in the race. I think for the next race I’m just going to do two packages of the Shot Blocks as I was a little uncertain about how much of the Honey Stingers I was getting in. The Shot Blocks are just nicely dosed, so they’re easier to manage. My time and splits: http://live.xacte.com/cleveland/?id=110&amp;tagcode=1458 ####Takeaways from the race: My training over the 4-6 months prior was worth all the work. In particular, I really believe the weekly speed/interval training that I did was key for me maintaining my pace. My nutrition was solid and I like the Clif Shots, so I think I may go to those only for a race. There’s still some room for improvement and I’m going to move up my new goal to 3:30: I’m going to move my training pace up by 30 seconds to 9:30 pace. I need to stay on track with my weekly speed work and look into potentially adding a second day per week of speed work. I need to find a way to train myself better both physically and mentally for those difficult miles: 21-25. Begin training for the 50-mile ultramarathon (My hypothesis is that by training for a longer race, I will be able to easily run a shorter one). Take the VO2 Max test","raw":null,"content":null,"categories":[],"tags":[{"name":"Marathon","slug":"Marathon","permalink":"http://ralucas.github.io/tags/Marathon/"},{"name":"Cleveland","slug":"Cleveland","permalink":"http://ralucas.github.io/tags/Cleveland/"},{"name":"Race Report","slug":"Race-Report","permalink":"http://ralucas.github.io/tags/Race-Report/"},{"name":"Running","slug":"Running","permalink":"http://ralucas.github.io/tags/Running/"}]},{"title":"How I've trained for a marathon","slug":"How-I-ve-trained-for-a-marathon","date":"2013-06-01T23:57:43.000Z","updated":"2015-02-02T01:07:24.000Z","comments":true,"path":"2013/06/01/How-I-ve-trained-for-a-marathon/","link":"","permalink":"http://ralucas.github.io/2013/06/01/How-I-ve-trained-for-a-marathon/","excerpt":"","keywords":null,"text":"I began really back in the summer of 2012, when I started training for the Columbus half-marathon and then the Whitney climb. After I ran the half, I continued running, and in talking with a co-worker who had experience in marathoning, I began planning a winter marathon. My initial plan was to run either the Jackson, MS or Baton Rouge/Louisiana Marathon, which were both taking place in January of 2013, I believe. So, I found Hal Higdon’s site and began following one of his plans. The site has plans for any level and a lot of great advice. So, I started increasing my mileage, eventually doing a 22 mile run. This was all in November/December of 2012. It was cold. I can remember doing the 22-mile run, not feeling super great upon embarkation, but doing it anyhow. And I was only taking with me a water belt, with one gatorade and one water bottle, and a peanut butter and jelly sandwich. Not enough really. So, by the end of the run, I was dead tired, leg muscles cramping, and actually had to stop and walk and few times (really no big deal, but still not what I was hoping for). I was unprepared and probably pushed my body when I wasn’t feeling it. During this time, I did decide to join a local running group, which began it’s ‘Winter Session’ in January. This was fine as I was going to do a spring marathon anyways and my goal was (and still is) 3 marathons in 2013. Joining the Marathoners-in-training group here in Columbus, for me, has been really great. I’ve learned a ton about best training practices and regimens, proper nutrition, pacing, and I’ve met a bunch of kind, like-minded folks. So, how I’ve trained: I started back in January for a May race, so 4 solid months, and I had some decent fitness already. If you don’t, you may want to give yourself some more time and begin to build a stronger base. My target pace is 8:30, which I think is doable, given my results in the half-marathon at a better pace (8:20). Now that’s not significantly better than 8:30, but I’m hoping that given my training, I can hit it. Prior to this January, I used to run at a much faster pace, 9-minute or faster. Now all my runs are usually at around 10-minute pace as it was recommended that the rule of thumb for pacing is to run most of your practice runs at 60-90 seconds slower than your goal marathon pace. One reason, I believe, for going a little slower is to build up your base. I tried a variety of foods while running: Clif Shot Blocks, GU, Gatorade gels, Gummy Bears, Honey Stingers, Peanut Butter and Jelly Sandwich, Chews, Buddy Fruits, Powerbars, and any number of protein bars. So, my favorites for running are the Clif Shot Blocks (usually Strawberry flavor) and the Honey Stingers. The GU and Gummy bears did not sit well with me, and I think they’re just too much sugar, too quickly, even if I do try to offset with some water. I could probably drink more water with them, but I just really like the shot blocks and stingers. I’m still struggling on finding the right pair of shoes. The primary reason is my feet are flat and so I overpronate. I’ve been using/comparing the Asics Gel Kayano and the Nike Lunar Eclipse. The real discovery for me has unfortunately come at the end of training, as now the ball of my right foot has begun to ache with the Metarsalgia I thought that I had gotten through. You can read more about it here. So, there you go. Let me know if you have any suggestions or comments on how I could be doing this better. Thanks.","raw":null,"content":null,"categories":[],"tags":[{"name":"Marathon","slug":"Marathon","permalink":"http://ralucas.github.io/tags/Marathon/"},{"name":"Cleveland","slug":"Cleveland","permalink":"http://ralucas.github.io/tags/Cleveland/"},{"name":"Running","slug":"Running","permalink":"http://ralucas.github.io/tags/Running/"}]},{"title":"Running Pains","slug":"Running-Pains","date":"2013-05-14T18:00:00.000Z","updated":"2015-02-02T01:02:19.000Z","comments":true,"path":"2013/05/14/Running-Pains/","link":"","permalink":"http://ralucas.github.io/2013/05/14/Running-Pains/","excerpt":"","keywords":null,"text":"I’m just going to preface this post by saying I’m not a doctor, so this is simply my own experience. If you’re feeling foot pain you should see a doctor. So, over the past year or so, since I’ve really started running in earnest and outside a lot more, I’ve developed an intermittent pain in the ball of my foot. Last year, it was on my left foot, but now it has migrated to my right foot. When it happened last year, I got worried that maybe it was a stress fracture, so I went to the podiatrist right away. He gave the foot an X-ray and it wasn’t fractured. He told me, he thought it was a neuroma or metatarsalgia. And suggested, given my flat feet, I really should get custom orthotics. I still haven’t done that. Nonetheless, I took some time off, began running more, and the pain really ended up going away. It didn’t come back until I decided, being the smart guy that I am, that I needed to break in a new pair of running shoes for the upcoming marathon. And so on about the 3rd or 4th run, I started getting that dull, ball of the foot pain around the 2nd and 3rd toe. A real bummer. And now as I’m getting ready to run here this coming Sunday, my foot is still bothering a bit. Hopefully, I’ll be fine for most of the race and I’m going to just tough it out if it does bother me. It’s really not bad, just sort of a bothersome ache/pain. Here’s what I’ve learned, but don’t hold me to this: Need to make sure the toe box in the shoe is good and roomy. I’m going to try a new tying method. A new pair of shoes can potentially cause re-inflammation. Don’t break in a pair of shoes with a month to go in training. I actually thought I’d be fine. Lesson learned. Start out with a couple (or more) pairs. I probably should have done a bit more cross-training near the end when my foot was bothering me and I’m going to try that this summer, should it happen again.","raw":null,"content":null,"categories":[],"tags":[{"name":"Running","slug":"Running","permalink":"http://ralucas.github.io/tags/Running/"},{"name":"Injury","slug":"Injury","permalink":"http://ralucas.github.io/tags/Injury/"}]},{"title":"Tough Mudder Review","slug":"Tough-Mudder-Review","date":"2013-05-01T23:46:15.000Z","updated":"2015-02-02T03:24:57.000Z","comments":true,"path":"2013/05/01/Tough-Mudder-Review/","link":"","permalink":"http://ralucas.github.io/2013/05/01/Tough-Mudder-Review/","excerpt":"","keywords":null,"text":"So, I did the Tough Mudder here in Ohio this past Saturday with my sister and a couple of people that joined our team after I put up forum post for team members. It was held in Mansfield, OH about an hour north of Columbus. My sister and I signed up for it, not really knowing much about it, back in the fall of last year. I had heard a friend mention it and I thought that it sounded like a good challenge. It costs us, I believe, around $120 to do the event. The Tough Mudder is basically an obstacles course with muddy terrain and is part of the growing phenomenon of these obstacle-type runs that seem to be popping up everywhere. It took place at what seemed to be a farm near Mansfield, Ohio. It’s a ten mile course and they had 22 obstacles. They range from crawling under barb wire for 15 feet to climbing over walls that are 15 feet to just running/walking through a muddy path. Some of the obstacles can be a bit tough, particularly as the day wears on. My sister and I signed up, but we needed to get more team members. Unfortunately, we couldn’t convince any of our friends to join us, so we put up a post on the forums and Adriana and Bob responded to join our team. We really couldn’t have asked for better teammates. They were great. Okay, so we trained and then finally came the day of the event. The emails from the organization told us to try to arrive 2 hours earlier than your start time. Being an unbeliever in showing up too early for anything, we blew that off and decided to show up an hour early. Well, we should have listened to them. There was a huge crowd, traffic, parking lines, and then we got bussed to the race field. We estimated that if they had about 9,000 participants per day going through the course. That’s quite a bit. We ended up arriving about 40 minutes late. So, we should have taken that 2 hours prior a bit more seriously. We really lucked out, though: it was a beautiful day - mid-60s and sunny. Upon arriving, we located Bob and Adriana and jetted off to start of the race. Before beginning, the MC gave a 20 minute speech, trying to rile everyone up, going over the standards and rules, and of course, not forgetting to mention all of the corporate sponsors. And then finally, we were off. The total time we spent out on the course was about 4-1/2 hours, which is basically the equivalent of a marathon time-wise. For the most part, we tried to run/jog between the obstacles, giving into a walk when the mud became particularly heavy. And the mud was heavy for the most part and it was everywhere. Tying your shoes on tight is essential if you want to keep them on. Bob was the only team member to have successfully completed all the obstacles. I felt like I did fairly well, only being unable to get past the monkey bars (my excuse: hands were too wet and slipped). Adriana completed the monkey bars successfully and was told that she was one of the few women to have accomplished that. And my sister had a lot of success and attempted every obstacle. Toughest obstacle: Berlin Walls - Part of the reason was that I initially attempted this solo and didn’t make it. Fortunately some fellows helped me over, but it was still rough pulling myself over the walls. As we neared the last 5 or 6 obstacles, lines began to grow and getting through the obstacles began to take more and more time. This was a bit of a detractor and I thought that perhaps they should begin to limit the amount of people on the course per day as it really seemed like too many. I will say this, I was sore for about two days after it and even now have large bruises under my arms from lifting myself up on the walls. In terms of training, the Tough Mudder website has some training suggestions, but I didn’t really attempt them. I felt that they were too difficult and crazy to attempt at the gym. My training was basically train for a marathon and I also did some weightlifting, swimming, and some of the DVD Insanity. So, I would do a workout of either weights, swim, or DVD in the morning and then do my mileage of running in the evenings. This is probably more than most people would have time to do, so I think that if you’re just maintaining a healthy running schedule with some weight training thrown in there, you would be fine and no need to workout twice a day, either. My conclusion: We had a fun time out there, got great weather, and met a couple of good folks. I don’t think I would do it again because, well, I guess I don’t see what the big deal about it is. Surely, it taxed me physically, but I didn’t feel like it was truly an intense physical experience. Certain obstacles were difficult, but the majority of them were very reasonable for anyone in decent shape to complete. If I was more flexible, the wall climbing may have been easier. I think, for myself, I didn’t really get that feeling of accomplishment and perhaps this was because I’ve had my sights on the completion of an upcoming marathon and then some mountain hiking. My recommendation would be, definitely do it if you feel like this would be something that you would enjoy and you’ve got a good group to do it with, but if you’re doing it to prove something to yourself, I think there are more difficult physical challenges out there. Thanks for reading this.","raw":null,"content":null,"categories":[],"tags":[{"name":"Running","slug":"Running","permalink":"http://ralucas.github.io/tags/Running/"},{"name":"Tough Mudder","slug":"Tough-Mudder","permalink":"http://ralucas.github.io/tags/Tough-Mudder/"}]},{"title":"Should you try to turn around a dying company?","slug":"Should-you-try-to-turn-around-a-dying-company","date":"2013-02-02T00:44:58.000Z","updated":"2015-02-02T00:45:21.000Z","comments":true,"path":"2013/02/01/Should-you-try-to-turn-around-a-dying-company/","link":"","permalink":"http://ralucas.github.io/2013/02/01/Should-you-try-to-turn-around-a-dying-company/","excerpt":"","keywords":null,"text":"Easy Answer: No. Hard Answer: No. Ego often times gets right in the way of properly answering this question. When someone calls you in and says, “We’ve got this company, it’s hit some hard times, do you want to see if you can turn it around.” And of course you say to yourself, ‘Who me? Of course, me, I can do it, I’ll save the day,’ all the while imagining yourself walking in to the company to the hero’s theme, wind flapping about your mane and cape, all the employees falling to their knees in praise, rushing you with their gratitude, your bosses nodding approvingly, and your adoring public cheering your name. And isn’t it a bit of a seductive fantasy? Sure and completely naive. But, you know it’s a fantasy after all. And yet you still think to yourself, I’m smart enough. So, you go in and do an analysis to decide if it can be turned around. Maybe you see some places where they can improve or you’ve got a plan that you’d like to put in place that has worked at previous businesses (see JC Penney). ‘Yeah, this can be done and my name will be etched in textbooks.’ You’ll come in and start instituting some of those changes. First, things don’t usually work that fast, so not much will happen. And there will be a ton of resistance from the current employees. [Aside: if you do decide to take over a sinking ship or you buy one, I would probably suggest that you learn as much as you can and unfortunately phase out a lot of the employees. It’s better for both parties as their resentment and resistance will only get worse for you and you will begin to resent and dislike them, creating a bad environment. You really need to create some of your own staff and I’m not saying you need a bunch of yes-men, just new blood to infuse the environment with some of the new energy that I assume you’re bringing.] You may tell yourself that you’re doing this for those employees. ‘Without this job, they wouldn’t be able to make it.’ What ego! And, you know, that’s what I do all the time. ‘It’s for others: the employees. They love working here. And with this economy like it is, they’ll only suffer. So, it’s for their benefit.’ Yeah, buddy, keep telling yourself that. You’re taking a big risk with a small payoff and probably a worse downside. It’s a gamble no smart betting man would take. So, if you do look at a company like this, make sure that you can make it profitable fairly quickly, else a heap of trouble is coming down the line. In the end, you’ll be better off and worse off. You didn’t sign up for a Peace Corps mission, but that’s what you got. So, if you really do want to do something maybe for some actual good, go and sign up with the Peace Corps. Just Don’t. Start your own business. Hire your own employees. And if it fails, have the guts to tell them and let them go.","raw":null,"content":null,"categories":[],"tags":[]},{"title":"Advice from my mistakes","slug":"Advice-from-my-mistakes","date":"2013-01-02T00:43:13.000Z","updated":"2015-02-02T00:43:57.000Z","comments":true,"path":"2013/01/01/Advice-from-my-mistakes/","link":"","permalink":"http://ralucas.github.io/2013/01/01/Advice-from-my-mistakes/","excerpt":"","keywords":null,"text":"Don’t stay in a job you don’t like. Quit or begin looking immediately. Even if you’ve only been there for less than year, get out. Always work hard and give your best, even if you don’t want to or others don’t seem to appreciate it. Don’t buy a house unless you have a family, need the room, and plan on staying in the same city for 10 years or more. Trust your gut. Read a lot of different books, fiction, non-fiction, religious, whatever floats your boat, read a book about it, then read about something else until you find something you’re interested in. Don’t watch too much television or Netflix. Get professional certifications early and make your company pay for them. They may be meaningless, but a lot of hiring folks think they’re something. Get outside and be active. Hike, camp, fish, run, play a sport, something, it doesn’t matter, just get out there. Stop reading the dopey news on the internet. It’s probably false anyways. Master something. Volunteer. Get a part-time job or start a side-business. Accomplish a big goal every year that requires preparation, e.g. run a marathon, start a company, etc. Don’t fret over your mistakes for too long. Take the hit (no matter how bad) and move on. Try to think outside yourself. How are others doing and dealing with life? It really takes the burden off.","raw":null,"content":null,"categories":[],"tags":[]},{"title":"Mt. Whitney via Mountaineer's Route","slug":"Mt-Whitney-via-Mountaineer-s-Route","date":"2012-10-01T23:37:15.000Z","updated":"2015-02-02T03:16:47.000Z","comments":true,"path":"2012/10/01/Mt-Whitney-via-Mountaineer-s-Route/","link":"","permalink":"http://ralucas.github.io/2012/10/01/Mt-Whitney-via-Mountaineer-s-Route/","excerpt":"Well, here I am on the summit of Mt. Whitney on a hike done in September 2012 via the Mountaineer’s Route. Unfortunately, given the oncoming clouds, we only had a minute or two to savor the accomplishment before we had begin the descent, but what a fun adventure! So, a little backstory here: In August, I had been dating a girl that I was beginning to like, and then suddenly she stopped calling me and only after probably too many texts on my part, she finally had the courage to send me the ol’ fashioned breakup text. Woe is me, right? I had to do something, so I looked online for a guided climb of Mt. Whitney (the tallest peak in the lower 48, oh yeah!). There didn’t seem to be one going around Labor Day when I was going to have a chance to take a trip, so I shot some guiding companies some emails. I contacted SWS Mountain Guides to see if I could get on a waitlist or if they had a guided hike going on around Labor Day (check them out here, they also do many other guided climbs and I think I’m going to do Shasta 2013 Summer - they were great, so I do recommend them). One of the owners responded and said that they were putting a group together for around that time. Next thing I know, he calls me, I give my credit card, and I’m signed up. “Shit!,” I thought, “now I’m going to Mt. Whitney, and I’ve never climbed a mountain, and really haven’t done too much backpacking. I’m just a guy from the suburbs in Columbus, Ohio. How is this going to go?” Fortunately, I had been doing some training for an upcoming half-marathon, so I wasn’t completely out of shape. But, still only 5 weeks to get ready. I stepped up my running and started backpacking with 30 lbs at the various parks around Columbus. I started to get a sharp pain in the ball of my foot from probably over-training. I suspect either Sesamoiditis or Metatarsalgia, but the podiatrist didn’t give me a diagnosis. Had an X-ray to eliminate whether or not it was a stress fracture and would it derail my planned trip (it wasn’t). Which would have been a real bummer, as I was now looking forward to it quite a bit. The doctor made up a custom little orthotic and I ended up just taking it a little easier and the pain began to subside. So, I was good for the trip.","keywords":null,"text":"Well, here I am on the summit of Mt. Whitney on a hike done in September 2012 via the Mountaineer’s Route. Unfortunately, given the oncoming clouds, we only had a minute or two to savor the accomplishment before we had begin the descent, but what a fun adventure! So, a little backstory here: In August, I had been dating a girl that I was beginning to like, and then suddenly she stopped calling me and only after probably too many texts on my part, she finally had the courage to send me the ol’ fashioned breakup text. Woe is me, right? I had to do something, so I looked online for a guided climb of Mt. Whitney (the tallest peak in the lower 48, oh yeah!). There didn’t seem to be one going around Labor Day when I was going to have a chance to take a trip, so I shot some guiding companies some emails. I contacted SWS Mountain Guides to see if I could get on a waitlist or if they had a guided hike going on around Labor Day (check them out here, they also do many other guided climbs and I think I’m going to do Shasta 2013 Summer - they were great, so I do recommend them). One of the owners responded and said that they were putting a group together for around that time. Next thing I know, he calls me, I give my credit card, and I’m signed up. “Shit!,” I thought, “now I’m going to Mt. Whitney, and I’ve never climbed a mountain, and really haven’t done too much backpacking. I’m just a guy from the suburbs in Columbus, Ohio. How is this going to go?” Fortunately, I had been doing some training for an upcoming half-marathon, so I wasn’t completely out of shape. But, still only 5 weeks to get ready. I stepped up my running and started backpacking with 30 lbs at the various parks around Columbus. I started to get a sharp pain in the ball of my foot from probably over-training. I suspect either Sesamoiditis or Metatarsalgia, but the podiatrist didn’t give me a diagnosis. Had an X-ray to eliminate whether or not it was a stress fracture and would it derail my planned trip (it wasn’t). Which would have been a real bummer, as I was now looking forward to it quite a bit. The doctor made up a custom little orthotic and I ended up just taking it a little easier and the pain began to subside. So, I was good for the trip. Lone Pine and the Portal Flew out to LAX, rented a car, and drove out to Lone Pine. I stayed in the Whitney Portal Hostel for the night (if you do go out there, it’s a nice, cheap place to stay on your way up or out). I think I was the last one in for the night. Everyone in the room kept to themselves and I struck up a short conversation with what seemed to be a drifter type character. He seemed nice, but told stories that had more of a tall tale feel. Wasn’t sure about him, so I spooned with my bag. Lone Pine is a tourist town, the main strip adorned with gift shops, restaurants, and bars. I’m not sure many people actually live in the town. I woke up early the next morning and walked across the street to the Mt. Whitney Restaurant for breakfast. It was good, standard breakfast fare. I decided to do some driving around and see the area. So, I drove out to the Alabama Hills, which are right there. A lot of westerns were filmed in the area. The Alabama Hills definitely have a distinct look. They are these beautiful, reddish-brown, sandy rocks jutting out of the desert, grouped and piled up together. The rest of the day, I spent mostly driving around and taking photographs of the scenery. I drove up to the Whitney Portal, checked it out. Drove down, back to town. Drove out to the main ranger station, picked up a map of the hiking routes and some post cards. Ate lunch in town at the local coffeeshop. I drove over to the Cottonwood Lakes area, which was actually a scarier drive and I think it may have been somewhere around 10,000 feet. That drive seemed to take a while, both up and down. Part of my reasoning for heading up there, along with checking out the area and having nothing else to really do, was to hopefully give my body some chance to acclimate to the higher elevation, given that I have lived my entire life at about 900 feet. In the late afternoon, I finally headed up to the Whitney Portal. I had planned on camping out at the portal in order to give myself a chance to acclimate. Individual campgrounds are right there, just off the parking lot and I think they’re about $5 or $10 for the night. There was really no one else around, so I had my pick of camping spots. So, I set up my tent and everything and took it easy for a little bit. Now, one thing I didn’t realize that I should have done (and I’m claiming ignorance here) is that your tent should be tied down to rocks. This is due to, what I believe, the fact that the ground is mostly rocky, not hardened dirt, and it can also be quite windy at the higher altitudes. There was a pile of rocks near the site and I looked at them somewhat bewildered, thinking that perhaps you put them on or in your tent to hold it down. I put a few rocks in the tent and on the corners. But, I think what you want to do is you want to tie some guy lines from your tent to the rocks to hold them down. So, anyways, I headed off to dinner at the Portal Store. The Portal Store is right at the start of the Whitney Trail. It’s a little shop for last minute supplies and it’s got a kitchen that serves gigantic burgers for lunch. Although I didn’t have breakfast, I believe they also make these huge, plate-sized pancakes. It’s a great stop after a long day of hiking or in my case of driving the twisting roads of the Sierras. So, I had a giant burger and fries. I tried to sit down, but was chased away by all the bees that were after my ketchup and I ended up inhaling my food in fear of an impending bee attack and got out of there. I took it easy the rest of the evening and the weather was beautiful. I sat and read a book by a babbling brook near the campsite. It was great. Day One - To Basecamp Woke up early, packed up my tent, threw everything into my rented Kia Soul and headed back down to Lone Pine to meet the group and the guides. We met outside of the local outdoor gear shop right in the center of town called Elevation Sierra Essentials. It’s owned by a fellow named John, who ended up being one of our guides, and is a truly kind, knowledgeable person. If you end up needing any gear before a Whitney hike, this is the place to stop into. We did a gear check with the guides, Tim and John, making sure we had everything we needed and if not we could just buy it next door. The group consisted of six hikers plus two guides. There was a father and son (Gabe and Michael), two old friends (John and Joann), a single hiker (Esther), and myself. So, we repacked our stuff, found parking for our cars, and we drove up to the portal to begin hiking. We started on the Whitney trail for only a little bit until the the Mountaineer’s Route breaks off. What makes it a route and not a trail is that it isn’t explicitly marked. You can take anyway to get there, it’s more of a guidance. So, once you break from the Whitney Trail, it almost immediately becomes more steep and some bouldering is involved. At this point, one of the hikers, John (who I believe may have had a bad knee) decided to head back. He was out for the trip. The hiking early on isn’t too bad, over some creeks and rocks, but as to be expected from a mountain climb, it is consistently steep. So, one technique that you want to utilize as much as possible is the mountaineer’s rest step. It’s essential for mountaineering. Basically: 1) you take one step forward, allowing your back leg knee to lock, keeping the weight on the back knee; 2) you then shift your weight to the front leg and step up with the back leg, keeping weight off the stepping leg, then locking the knee of the rear leg (back to step one). This allows you to take tiny rests as you are climbing. It also slows you down a little bit and as things get steeper, the amount of steps taken may slow down. There was some minor bouldering and climbing over rocks, but nothing of note until you come upon the Ebersbacher Ledges. Ebersbacher Ledges The Ebersbacher Ledges consist of a quick traverse that is exposed and then climbing of a series of a few accessible ledges until you are clear. The difficulty with the ledges is primarily that of exposure. It’s not serious climbing, it’s simply that a serious loss of balance would end badly. I also think part of it is a bit of a self-fulfilling prophecy. Everyone says they’re tricky and frightening, so they become that way. So, they’re not difficult to climb in a technical, they’re just difficult in the face of exposure. The key, obviously, is steadiness. One thing to note is that, you do need to know where to go up, as from some stories John was telling me, there have been hikers who have continued across the face and realized that they had passed the ledges to get up and then had gotten either stuck in a bad place or had to backtrack. So, getting the Ebersbacher Ledges, we then proceeded across the traverse one-by-one and then up the ledges. One of the hikers got a little spooked, and I felt the same way, so one of the guides coaxed each of us across the traverse to the ledges. It was then up the ledges, hike across a tree, and then up a few more and we were done. We then continued to hike up to Lower Boy Scout Lake. Unfortunately during the hike it began raining, so on came the rain coats and pants, and with it still being quite warm, it was a bit uncomfortable. We got to Lower Boy Scout Lake for lunch and fortunately found a bit of cover to sit and relax as it had begun raining much harder at this point. Lower Boy Scout Lake lies at about 10,300 feet and is still mostly within the tree line. It’s a beautiful spot to sit and relax after a morning’s vigorous hike. I downed a couple of Clif bars (which by the end of the hike I was thoroughly tired of eating. It’s sort of like Champagne, if you have too much you’re going to swear it off for good. That’s me and the peanut butter flavored Clif bars, I just had too many that week) and off we went. Up the mountain, over some water run offs, it can get a little slick, so footing continues to be key and a good pair of hiking poles are nice to have. We climbed another 1,000 feet or so, to our base camp at Upper Boy Scout Lake (~11,300 feet). A tent had already been setup prior to our arrival, most likely from a previous guided climb, and staked out a great spot on the rocks. So, we set up two more tents, dried our stuff off a bit and made a few phone calls (yes, there is cell phone reception up there, wow!?!). Upper Boy Scout Lake is a good spot to camp on this hike, although if you do want to go further, you can make it to Iceberg Lake in time if your pace is good. John then took us out and gave us a tutorial on rope climbing and belaying basics with a group of climbers. We came back, ate some noodles for dinner, and went to bed as the plan was to get up early (2 or 3 am) for the hike to the summit. The sky was clear that night, so I took some time to lay out on a rock and stare up at the stars. Wonderful. To setup a tent where there is no good clay to hold stakes, the best way is to guy line the tent to heavy rocks. It’s also quite windy on Mt. Whitney, so it’s just a good idea. Here’s a quick tutorial on how to tie a taut line: http://youtu.be/ihL5cnj5nKU Day Two - Summit We awoke at 3:30am, a little later than we wanted, tossed on our headlamps, threw on our packs, and headed up the trail to Iceberg Lake (~12,600 feet). The night prior, we unloaded most of the unneccessary stuff from our packs for the summit hike. I had asked Tim why it was that we didn’t need the smaller “Summit Packs” and the fact that they wanted us to carry our large packs and the reason he gave, was that the full packs are better at protecting your entire back should you fall back or hit a rock. It was relatively cold in the early morning, so we warmed up quickly. Because of the late start, Tim, the main guide, pushed the pace a bit. It wasn’t too bad, but some of the folks in the group began slowing up. The hike wasn’t too bad, with a bit of rock scrambling, but nothing overly strenuous. Not until we got to Iceberg Lake, did the climb begin to take on any real difficulty. This was where the true climb began. We arrived around 8am, rested and ate, before beginning the final ascent. Joann at this point began not feeling well and decided not to join us in the climb. For the first part of the climb, it was heavy bouldering. My hands and legs got a bit chewed up from the coarse rock, but nothing terrible. Tim, Gabe, and I began to move a bit quicker than Michael and Esther at this point (John was guiding them), so Tim began pushing us at a quicker pace to attempt a summit climb as clouds were seen moving in. After a couple of hours of scrambling, we got to a rest point called “Piss Rock,” for obvious reasons. Took a short breather and then continued heading up. There was very little snow on the mountain, it was mostly gravel, so footing became an issue and moving quickly was apparent. It was definitely exhausting, maintaining such a solid pace. After what I believe was an hour or two more, we reached the “Notch.” It was here that we made the true final climb to the summit of Mt. Whitney of about 500 feet. We again took a brief breather and had something to eat. First, you have to scramble up a brief precipice before getting to a spot where you mostly just climb up rocks. Tim first scrambled up and got some rope into place. Gabe and I then hooked up and scrambled up. Not terrible, we just had to move quickly. At this point, Tim would go on ahead, get the rope fixed for “on belay” and then we would follow. There were a few blind rocks where we had to pull ourselves up, but nothing like rock climbing a face. Tim was trying to getting us up there quickly as a cloud was moving in and he wanted to get us up and off the summit in order to avoid any chance of lightning strikes. Well, we made it. We got a few minutes at the top. I remember the feeling of elation of reaching the summit. What a good feeling to achieve the summit. I wanted to run over to the house and sign the book, but Tim was obviously nervous at this point and wanted to get us down. I don’t blame him as he is charged with responsibility for us, “the clients.” So, down we went. Unfortunately, Esther and Michael didn’t get the chance to go for the summit as Tim called them off given the oncoming clouds. I actually found the climb down from this point to be more difficult than the climb up as getting my footing was at times blind and uncertain. And as they say “49% is getting to the top.” At the scramble part, as I was trying to slowly ease myself down, I fell. Fortunately, I was on rope, so Tim caught me. It could have been quite ugly. Thank you, Tim. From the Notch down, it wasn’t too bad. Footing really was the issue as we began the descent as the rock is quite loose in this section. Another few more hours and we were back at Iceberg Lake, with the most difficult portion of the climb past us. We took a nice break. At this point it was the afternoon, so we started out to Upper Boy Scout Lake to get back for dinner and rest. At this point, I began dragging a bit and not hiking as quickly. Nearer to Upper Boy Scout Lake there is a large amount of quartz rock. Day Three - Back to Lone Pine We woke up early to a beautiful sunrise over the Sierras, ate a nice hot breakfast of oatmeal, and headed down. We didn’t have to pack up the tents as a group was heading up that day to utilize them, which was a nice bonus. The hike down wasn’t too bad, I was just a bit worn out from the previous day, so my technique at times got bad, allowing my front knee to lock up. Not a good idea and an easy way to injure oneself especially with a 30-40 pound pack. The day was great and as we descended, it got progressively warmer. Down at the Ebersbacher Ledges, I had a bit of moment of freak-out and began to get nervous, got myself a little stuck and uncomfortable on them. Made it through them, though, with some coaxing from John (thank god for John). Unfortunately, I think my boots may have been too small and didn’t leave enough toe room, so I got a serious case of toe bang and lost a few toe nails to it. I really should have been wearing about a half size larger to give my toes so more room. Lesson learned. Well, we got down to Portal, took a few photos and headed back to Lone Pine. We all ate a celebratory lunch at the Mt. Whitney Restaurant, enjoying a nice big burger and beer. I was so worn out that I decided to stay the night in Lone Pine before heading to the coast for some surfing and relaxing on the beach. Further Reading on Mt. Whitney Here’s a link also to another great blog regarding hiking the Mountaineer’s Route of Mt. Whitney that I found helpful before I went: http://www.sierradescents.com/climbing/whitney/mountaineers-route.html","raw":null,"content":null,"categories":[],"tags":[{"name":"Mt. Whitney","slug":"Mt-Whitney","permalink":"http://ralucas.github.io/tags/Mt-Whitney/"},{"name":"Mountaineer's Route","slug":"Mountaineer-s-Route","permalink":"http://ralucas.github.io/tags/Mountaineer-s-Route/"},{"name":"Mountaineering","slug":"Mountaineering","permalink":"http://ralucas.github.io/tags/Mountaineering/"},{"name":"Climbing","slug":"Climbing","permalink":"http://ralucas.github.io/tags/Climbing/"}]}]}