{"meta":{"title":"Richard Lucas' Blog","subtitle":"Node.js and more...","description":"Node.js and more...","author":"Richard Lucas","url":"http://ralucas.github.io"},"pages":[],"posts":[{"title":"Activity Classification using MHI Techniques","slug":"Activity-Classification-using-MHI-Techniques","date":"2017-12-03T07:00:00.000Z","updated":"2018-01-08T02:36:31.458Z","comments":true,"path":"2017/12/03/Activity-Classification-using-MHI-Techniques/","link":"","permalink":"http://ralucas.github.io/2017/12/03/Activity-Classification-using-MHI-Techniques/","excerpt":"","text":"Activity classification by computers has a rich history and this paper seeks to describe, analyze, and compare some methods used in classifying a subset of actions. The creation of temporal templates via motion-energy images (MEI) and motion history images (MHI) were utilized to analyze and quantify frames of videos. Image moments were taken from the MHIs and a learning model was built. The results obtained are compared and discussed along with other comparative methods. The activities focused on were boxing, handclapping, handwaving, running, jogging, and walking. Interesting Links Multiple Activity Videos: Handclapping &amp; Boxing Video https://youtu.be/dEtfgAYwCg4 Multitude activity Video https://youtu.be/x_hFKZzc0jw Related work in recently published researchThere are a number of research publications that are directly related to activity classification, although some of their methods may vary. The most pertinent related work to this project is that of Bobick and Davis [5], in which the idea of using temporal templates to classify activities is discussed. This technique relies primarily on the creation of Motion-energy images and Motion-history images to create a temporal template. Image moments are then extracted from these templates and a distance model is created. An alternative to temporal templates is suggested in Schuldt, et al [8] in which the use of space-time features are used with a support vector machine classifier. The space-time features are, in essence, points of interest that occur at specific points over time allowing patterns to be developed. They are constructed through the use of image gradients and a Gaussian convolution kernel in which points of interest are located in comparison to a maxima. The use of SVMs in training a model are then taken to achieve fairly good results (and will be compared later herein). In more recent work, the use of still poses and poselets (body part detection from joint and part positions) [7] to develop an underlying stick figure representation found good results in identifying activities. Additionally, this research took into account the 3D orientation from a trained model. The method achieved results of ~60% correct identification with just the poselet representation and ~65% when the object model was added. Another recent study [3] utilized summary dynamic rgb images and convolution neural network (CNN) to develop a system in which identification was done quickly and reasonably successfully as the researchers were able to get ~89% successful identification. Methodology and implementation usedTemporal templates were used to create modeled images over a period of time, traditional image moments [9][4][6][2] were then taken and a model was trained. Temporal TemplatesTwo types of temporal templates were created: motion-energy images (or binary images) (MEIs) and motion-history images (MHIs). In creating these images, each video frame was taken through a preprocessing step in order to better extract the purposeful, activity related motion from the background and tangential motions within each frame. This was done b first grayscaling each frame, then running the frame through OpenCV Gaussian blurring and morphing functions. The MEIs were created by taking a temporal diff from each frame and assigning coordinates with a 0 or 1 given if the temporal difference was greater than a constant $\\theta$ or not (see examples in figure 1). A $\\theta$ of $14$ was used throughout the training and testing phases as it was determined to give the best results. Figure 1: Clockwise, starting in left corner: Boxing, Handwaving, Handclapping, Running, Jogging, Walking The MHIs were built from those binary images and provide a more nuanced and graded view of the motion over time. These were calculated utilizing hand-crafted constants $\\tau$, in which over time gradients were developed. Figure 2 shows an example of different people handwaving as it shows more of the change over time versus the constant that the MEIs show. In creation of these temporal templates, the $\\tau$ constant value for each activity was additionally used (Table 1) a demarcation for how many frames to analyze from a video, so the video sequences (as outlined in the sequences[1]) were divided further into small $\\tau$ based chunks. This allowed for consistent training of an activity and expanding the total number of training samples, therefore giving more to the trained model to improve upon prediction. Activity Tau boxing 7 handclapping 13 handwaving 17 jogging 21 running 11 walking 35 Table 1 Figure 2: Clockwise, starting in left corner: Boxing, Handwaving, Handclapping, Running, Jogging, Walking Image MomentsImage moments for each MHI were taken to develop a feature set upon which the temporal templates could be compared. The central ($\\mu$) and scale invariant ($\\nu$) moments were calculated for the following moments $pq \\in {20, 11, 02, 30, 21, 12, 03, 22}$ [4]. The 7 main Hu Moments [6][2] were then calculated from the scale invariant moments. However, after much testing and training, it was determined that the scale invariant moments gave the best results. Training and PredictionA large dataset of videos [1] was used as input for training. The K-nearest neighbor algorithm was used to train, classify, and predict the videos. The K-nearest neighbor algorithm works by classifying input based on a plurality vote of nearest neighbors. The features used for training and prediction for this dataset was found to be a combination of the scale invariant features ($\\nu$) and the Hu moments. The primary factor here was, in using the Hu moments as features, that enough differentiation was not provided and there was an increase in false positives when testing. Results and performance statisticsFigure 3: Training results | 90.8% accuracy Figure 4: Test results | 84.1% accuracy Figures 3 and 4 give a confusion matrix on the overall results from training and testing. ExperimentIn addition to the single activity videos used to train and test against, videos with multiple activities within the same video were run as an experiment (sample frames in Figure 6). The method used for each multiple activity video did the following: Labels = Dictionary of labels $\\rightarrow$ $\\tau$ values initialize$Frame_List=1 + max(\\tau)$ As outlined in Algorithm [dmap], multiple activity prediction, first gets an initial prediction by building an array of frames. Then as that frame list is iterated over and $mei$ and $mhi$ images are built, when the number of frames is equal to a $\\tau$ value, a prediction from the predict engine is created based on the current $mhi$ values and that prediction is compared to the current $\\tau$ label. If they are equal, then that label is returned and a $\\tau$ number of frames is written and classified with that label. Analysis of resultsThe six activities could be further categorized into two primary groups: stationary activities and moving activities. Given this, focus can be placed on results within these categories. The most glaring issue is that of the handclapping results. The trainer most commonly confused it with handwaving and boxing, which does have similar motions. The moving activities also were difficult to separate, and particularly running and jogging proved to be quite difficult to distinguish between. However, given reasonably differentiated $\\tau$ values, the variation was carried through. It could also be argued that the difference between jogging and running may be quite different from person to person, so I still believe that these two classifications are somewhat interchangeable. Analysis of experimentAs shown in Table 2, the performance times of algorithm used in the experiment was fairly slow. It took close to half a second to complete a full run of the algorithm of frame set analysis and write, which entails running the algorithm (see above) and write the newly labeled frames to video. Method Time (s) Frames (36) analysis and prediction 0.45s Moments calculation 0.013s Temporal image creation 0.15s Table 2 The accuracy of the multiple activity videos was hit-or-miss. For example, in the video of just handclapping and boxing, the accuracy was outstanding, showing an above 90% accuracy (see Figure 5). However, in the multitude activity video, accuracy declined. Some of this was due to the definition of jogging versus running, but there were also misclassifications (see Figure 6). Throughout the multiple activity videos, there were misclassifications (see Figure 11), mostly among their two major groups (as discussed above), i.e. walking getting classified as jogging, boxing as handclapping, etc. Figure 5: Handclapping and Boxing | 90.7% accuracyN.B. $nan$ means that label was not expected Figure 6: Multitude | 36.4% accuracyN.B. $nan$ means that label was not expected Figure 7: Clockwise, starting in left corner: Boxing (correctly identified), Handwaving (incorrectly identified as clapping), Boxing (incorrectly identified as waving), Running (incorrectly identified as jogging), Walking (incorrectly identified as jogging), Handwaving (correctly identified Analysis of weaknessesThere were some weaknesses that came to light in running thesepredictions: Currently the method requires differing $\\tau$ values or if they are similar, quite different $mhi$ images. All the $\\tau$ values were carefully set and vetted. Simply not a scalable process. As the discussed above, the performance speed is quite slow. Still a fairly custom and brittle development process. Comparison to the state-of-the-art methodsIn comparison of the methodology used to the state-of-the-art methods, we can look at the results obtained from usage of an identical dataset from the Schuldt, et al [8]. In comparing the confusion matrices from the Local SVM Approach to this one, the results obtained from the usage of MHI were quite better and more consistently accurate. Method Accuracy This paper 84.1% Bilen et al. [3] 89.1% Zha et al. 89.6% …others ~88.0% Table 3 Proposals for improvementThere are four proposals for improvement that I believe can improve the performance and success rate of predicting activities: Utilizing hidden markov models as a part of the prediction process, particularly in prediction of multiple activities. This would additionally require more training input. Using poselets and stick figure representations as used in referenced work [7]. Building and utilizing convolutional neural networks, similar to the referenced work [3]. More and variegated training input videos. References[1] Action videos. http://www.nada.kth.se/cvap/actions/.[2] Hu moments | opencv documentation. . [Online; accessed 2-December-2017].[3] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, and S. Gould. Dynamic image networks for action recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3034-3042, June 2016.[4] A. F. Bobick. 8D-L2 activity recognition | CS6476 udacity lectures. https://classroom.udacity.com/courses/ud810/lessons/3513198914/concepts/34988000940923.[5] A. F. Bobick and J. W. Davis. The recognition of human movement using temporal templates. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(3):257-267, Mar 2001.[6] Ming-Kuei Hu. Visual pattern recognition by moment invariants. IRE Transactions on Information Theory, 8(2):179-187, February 1962.[7] S. Maji, L. Bourdev, and J. Malik. Action recognition from a distributed representation of pose and appearance. In CVPR 2011, pages 3177-3184, June 2011.[8] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: a local svm approach. In Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., volume 3, pages 32-36 Vol.3, Aug 2004.[9] Wikipedia. Image moments | wikipedia, the free encyclopedia. https://en.wikipedia.org/w/index.php?title=LaTeX&amp;oldid=413720397, 2017. [Online; accessed 2-December-2017].","categories":[],"tags":[{"name":"Activity Classification","slug":"Activity-Classification","permalink":"http://ralucas.github.io/tags/Activity-Classification/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"http://ralucas.github.io/tags/Computer-Vision/"}],"keywords":[]},{"title":"Actor Model in Erlang","slug":"Actor-Model-in-Erlang","date":"2017-01-09T08:13:46.000Z","updated":"2017-01-09T08:17:36.000Z","comments":true,"path":"2017/01/09/Actor-Model-in-Erlang/","link":"","permalink":"http://ralucas.github.io/2017/01/09/Actor-Model-in-Erlang/","excerpt":"","text":"ConcurrencyDefinition: The ability of an executing program to run decomposed instructions in a parallelized fashion efficiently utilizing available resources. From Joe Armstrong [10]: The way to phrase it, in my mind, is parallelism differs in that it is a purely physical difference, i.e. running the same program at the same time across similar devices, whereas concurrency infers running on the same machine (i.e. use of threads). But for the sake of this discussion, they may be used interchangeably Traditional ConcurrencyProgramming languages typically offer concurrent operation through the use of threads. Given the lack of atomicity in the use of threads, locking primitives such as semaphores or mutexes are required. This can create difficult to program and debug situations, such as deadlock. For brevity, this survey assumes some knowledge of current concurrency models. Thinking about ParallelizationObviously, computing speeds have continued their march to the beat of Moore’s Law, and given the complexity in writing concurrent programs that take advantage of the processors and architectures at the time, it’s not always worth it to consider writing software in this way. But there’s speculation that we may begin to see the perhaps potential physical limitations: there’s only so many transistors you can continue to fit into ever smaller physical areas. So, the use of parallelization becomes more and more attractive. Amdahl’s Law basically gives a good rule of thumb for how parallelized a program can get [My definition]: A program is only parallelizable in terms of it’s slowest part. Actor ModelActors are a concurrency primitive that do not share resources with another actor. They share state/data via message passing. An actor is an entity that has a mailbox and a behavior. It takes a message and can then send messages to other actors, create new actors, and return another actor describing its next behavior. Concurrency in actors is constrained only by the availability of hardware resources and by the logical-dependence inherent in the computation. - Gul Agha [1] The actor model is a framework to think about, model, and build distributed concurrent systems. Quick HistoryThe Actor Model was first proposed by Carl Hewitt in 1973 (yeah, the actor model is fully entrenched in middle age :)) It was further solidified over many years by a number of other computer scientists and by Gul Agha in his disseration on the Actor Model in 1985 [1]. Important Concepts No shared state between actors Functions asynchronously All about message passing for communication Only way to share/change state is through this message passing Enter Erlang…The actor model is inherent to Erlang and is built into the language and somewhat fundamental to understanding the distributed nature of Erlang. One thing to note, is Erlang was designed for the telephony industry, so some of the driving ideals such as reliability demanded that Erlang utilize a model that didn’t incur potential for data loss, that is possible in the more traditional concurrency models discussed above. An Erlang program describes a series of functions. Each function uses pattern matching to determine which function to execute.[6] In Erlang, each thread of execution is a lightweight process.[4] Erlang utilizes pattern matching to process messages. The Erlang view of the world is that everything is a process and that processes can interact only by exchanging messages. - Joe Armstrong [7] Creating a processTo create a process in Erlang, just call spawn, which will return a process id (pid) (refer to the Erlang docs for the signature of spawn you want). Here, the example uses spawn/3 (spawn(Module, Function, Args) -&gt; pid()) [5]1234-module(actor_file)....start() -&gt; spawn(actor_file, run, []). Calling start/0 here returns the pid:12341&gt; c(actor_file).2&gt; Pid = actor_file:start().3&gt; Pid.&lt;0.80.0&gt; But it’s not a process in the OS definition. It’s an Erlang process, so it runs in the Erlang VM. It runs in user space, not bound to the kernel, so is scheduled by the Erlang Scheduler. And it’s a lightweight process…how lightweight: A newly spawned Erlang process uses 309 words of memory [4] From the Erlang docs on memory: The unit of measurement is memory words. There exists both a 32-bit and a 64-bit implementation. A word is therefore 4 bytes or 8 bytes, respectively. [6] In fact, let’s check how much memory our process(es) are using:12&#123;_,Bytes&#125; = process_info(actor_file:start(), memory). &#123;memory,2720&#125; And they are very quick to spawn…from Joe Armstrong: Spawning 20,000 processes took an average of 3.0 microseconds/process of CPU time and 3.4 microseconds of elapsed (wall-clock) time. How Erlang schedules a processErlang uses a preemptive scheduler. A preemptive scheduler works by running tasks, preempting, then resuming them based on a specific metric, such as priority, time or reduction. Many other async languages, such as Node.js and Python’s Twisted, utilize cooperative scheduling, which requires tasks to release themselves voluntarily. Erlang’s preemptive scheduler uses a reduction count. A reduction is typically a function call, garbage collection, message sending, etc. The reduction count for an Erlang process is 2000, which can go quite quickly. Additionally, processes do and can be flagged with priorities: ‘max’, ‘high’, ‘normal’, and ‘low’. The scheduler makes Erlang a great choice for low-latency, parallelized programs as it can manage multi-tasking efficiently, given that it doesn’t allow any one process to monopolize resources. References and Further Reading[1] Agha, Gul, Actors: A Model of Concurrent Computation in Distributed Systems, http://www.cypherpunks.to/erights/history/actors/AITR-844.pdf, 1985.[2] Vermeersch, Ruben, Concurrency in Erlang &amp; Scala: The Actor Model https://rocketeer.be/articles/concurrency-in-erlang-scala, 2009.[3] Hebert, Fred, The Hitchhiker’s Guide to Concurrency, Learn You Some Erlang: for great good!, http://learnyousomeerlang.com/the-hitchhikers-guide-to-concurrency, 2013.[4] Erlang/OTP System Documentation, http://erlang.org/doc/index.html, 2016[5] http://erlang.org/doc/reference_manual/processes.html[6] http://erlang.org/doc/efficiency_guide/advanced.html[7] Armstrong, Joe. Programming Erlang: Software for a Concurrent World, 2nd Ed, 2013.[8] Tate, Bruce. Crossing borders: Concurrent programming with Erlang, http://www.ibm.com/developerworks/java/library/j-cb04186/j-cb04186-pdf.pdf, 2006.[9] https://hamidreza-s.github.io/erlang/scheduling/real-time/preemptive/migration/2016/02/09/erlang-scheduler-details.html[10] http://joearms.github.io/2013/04/05/concurrent-and-parallel-programming.html[11] http://yosefk.com/blog/parallelism-and-concurrency-need-different-tools.html[12] Andersen, Jesper Louis&gt; How Erlang does scheduling, http://jlouisramblings.blogspot.co.uk/2013/01/how-erlang-does-scheduling.html, 2013.","categories":[],"tags":[{"name":"Erlang","slug":"Erlang","permalink":"http://ralucas.github.io/tags/Erlang/"},{"name":"Actor Model","slug":"Actor-Model","permalink":"http://ralucas.github.io/tags/Actor-Model/"},{"name":"Concurrency","slug":"Concurrency","permalink":"http://ralucas.github.io/tags/Concurrency/"},{"name":"Distributed Computing","slug":"Distributed-Computing","permalink":"http://ralucas.github.io/tags/Distributed-Computing/"}],"keywords":[]},{"title":"Writing a small socket server in C","slug":"Writing-a-small-socket-server-in-C","date":"2016-08-12T05:15:22.000Z","updated":"2016-08-12T05:27:11.000Z","comments":true,"path":"2016/08/11/Writing-a-small-socket-server-in-C/","link":"","permalink":"http://ralucas.github.io/2016/08/11/Writing-a-small-socket-server-in-C/","excerpt":"","text":"Building a socket server in C is a great introduction to the language as well as IPC mechanisms such as sockets and it can show you how simple it is to set up a server. For this installment, we’re going to do a single-threaded version. So, let’s get started… The ServerFirst I want to define some constants that we’ll use throughout. For some of these you may want to pass these in dynamically, but for this case I’m just hardcoding them here as constants: 123456#define SOCKET_ERROR -1#define MAX_PENDING_CONNECTIONS 10#define LOCALHOST &quot;127.0.0.1&quot;#define PORT 8080#define BUFSIZE 1024 We’ll put the server in a void function that we’ll call serve: void serve(), so12void serve() &#123; // Put all subsequent code here Create the server socket file descriptor, usually created with with PF_INET, SOCK_STREAM, and with tcp IPPROTO_TCP. 1234int server_socket_fd;if ( (server_socket_fd = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP)) &lt;= SOCKET_ERROR ) &#123; perror(&quot;Error creating the server socket&quot;);&#125; Set the socket options, passing in the newly created socket file descriptor. Additionally, 12345int sockopt;int socket_opts;if ( (socket_opts = setsockopt(server_socket_fd, SOL_SOCKET, SO_REUSEADDR, &amp;sockopt, sizeof(int))) &lt;= SOCKET_ERROR ) &#123; perror(&quot;Error setting the socket options&quot;);&#125; Now we need to set up the socket address: 12345678910// Create ip addressin_addr_t ip_address;ip_address = inet_addr();struct sockaddr_in socket_address;// Allocate some memory for the socket_addressmemset(&amp;socket_address, 0, sizeof(socket_address));socket_address.sin_family = AF_INET;socket_address.sin_addr.s_addr = ip_address;socket_address.sin_port = htons(PORT); Bind the server 1234int server;if ( (server = bind(server_socket_fd, (struct sockaddr *) &amp;socket_address, sizeof(socket_address))) &lt;= SOCKET_ERROR ) &#123; perror(&quot;Error binding the server&quot;);&#125; Listen for incoming connections 1234int connection_listener;if ( (connection_listener = listen(server_socket_fd, MAX_PENDING_CONNECTIONS)) &lt;= SOCKET_ERROR ) &#123; perror(&quot;Error connecting the listener&quot;);&#125; Now let’s run the server by running an infinite loop 12345678910111213141516171819202122232425262728293031for (;;) &#123; int client; struct sockaddr_in client_address; socklen_t client_len; client_len = sizeof(client_address); // Accept incoming connections from the client if ( (client = accept(server_socket_fd, (struct sockaddr *) &amp;client_address, &amp;client_len)) &lt;= SOCKET_ERROR ) &#123; perror(&quot;Error accepting on the server socket&quot;); &#125; // Receive the message int receiver; if ( (receiver = recv(client, buffer, BUFSIZE - 1, 0)) &lt;= SOCKET_ERROR ) &#123; perror(&quot;Error receiving message from the client); &#125; // Respond ssize_t sender; char response[BUFSIZE]; size_t len; strcpy(response, &quot;This is a response string!&quot;); len = strlen(response); if ( (sender = send(client, response, len, 0)) &lt;= SOCKET_ERROR ) &#123; perror(&quot;Error sending the response&quot;); &#125; fprintf(stdout, &quot;Successfully sent (%i) message&quot;, sender);&#125; Go ahead, build it, and fire it up and send a curl request. In a future post, I’ll build an HTTP server. The source code for this can be found here: https://github.com/ralucas/basic_c_socket_server I also suggest taking a look at Beej’s Guide to Networking","categories":[],"tags":[{"name":"sockets","slug":"sockets","permalink":"http://ralucas.github.io/tags/sockets/"},{"name":"C","slug":"C","permalink":"http://ralucas.github.io/tags/C/"},{"name":"sys/socket.h","slug":"sys-socket-h","permalink":"http://ralucas.github.io/tags/sys-socket-h/"}],"keywords":[]},{"title":"Understanding PortLand","slug":"Understanding-PortLand","date":"2016-04-28T06:00:00.000Z","updated":"2016-05-04T03:31:44.000Z","comments":true,"path":"2016/04/28/Understanding-PortLand/","link":"","permalink":"http://ralucas.github.io/2016/04/28/Understanding-PortLand/","excerpt":"","text":"What is PortLandToday, most web applications are hosted in the “cloud”, which is in turn hosted in huge datacenters located in various places all over the world. Modern datacenters today suffer from a few limitations including: lack of scalability, difficult management, inflexible communication, and limited support for virtual machine migration.[1] PortLand is a proposal that attempted to solve some of these limitations. And what PortLand proposes is a solution to mitigate these issues by delivering scalable layer 2 routing, forwarding, and addressing for large data center networks. Portland utilizes the three tier fat-tree topology of edge hosts connected to aggreggation hosts which are connected to core hosts (Figure 1). Why Layer 2In large data centers, certain requirements come into being: Easy VM migration Need for less active administration and configuration Efficient end-host to end-host communication No forwarding loops Rapid and efficient failure detection Layer 3, IP, doesn’t enable easy VM migration as migration requires that VM’s change their IP addresses. Additionally, configuration of a Layer 3 data center is onerous given that it requires each switch to be configured and DHCP servers to be synchronized. Layer 2, while not the silver bullet, is still more able to satisfy these requirements under the Portland scenario. Portland is able to solve these issues through the use of Pseudo MAC addresses, controlled by a centralized mechanism (Fabric Manager), enabling efficient, loop-free forwarding with little necessary state management. Fabric ManagerThe fabric manager is a centralized (user) process running on a dedicated machine that maintains a soft state about the network configuration. These responsibilities include information about the topology, assisting with ARP resolution, fault tolerance, and multicast. The use of soft state is important here as it eliminates the need for active administration and replication is simpler as state does not need to be replicated as well. What the Fabric Manager (FM) DoesThe fabric manager reduces broadcast overhead in the common ARP request cases. So, when an edge switch intercepts a ARP request for IP to MAC mapping, it forwards the request onto the fabric manager, and the FM checks the PMAC table (see below), if the entry is there, it gives the PMAC back to the edge switch which creates the ARP reply and sends it to the original host. VM MigrationAdditionally, when doing VM migration, the VM will send out an ARP with it’s new IP to MAC mapping, which gets forwarded to the FM. The FM, in an effort to invalidate any cache that hosts may have regarding the newly migrated VM, sends out an invalidation message to the migrated VM’s previous switch. LocationThe fabric manager is located in the logical center of the data center. It can also be located Pseudo MAC AddressesIn order to efficiently forward and route as well as easily migrate VMs, Portland utilizes the concept of heirarchical Pseudo MAC Addresses (PMACs). Each end host in the data center gets a PMAC and in turn the PMAC encodes the location of the host in the DC topology. Components of PMACThe 48-bit PMAC is composed of 4 parts in the form of [pod].[position].[port].[vmid], so: Pod (16bits): The pod number of the edge switch Position (8bits): Hosts position in the pod Port (8bits): Switch-local view of the port number the host is connected to VMID (16bits): Used to multiplex multiple VMs on the physical box. This constructed PMAC ([pod].[position].[port].[vmid]) is then entered into the switch’s local PMAC table that is mapped to the hosts Actual MAC address (AMAC) and IP address. This mapping then is propogated to the Fabric Manager and it uses it to respond to ARP requests. Additionally, the switch creates a flow table entry to rewrite PMAC destination address to the AMAC for traffic headed to that host. Why PMAC is useful PMAC addresses enable efficient, provably loop-free forwarding with small switch state. Given that traditional layer 2 based centers have scalability and efficiency issues due to the need to support broadcast (usually via ARP). It also requires that switches maintain large MAC forwarding tables that can have 100,000 to millions of entries, requiring memory and software that just isn’t practical in today’s switch hardware landscape. Location Discovery ProtocolThe Location Discovery Protocol (LDP) is used in the assignment of the PMAC. LDP in turn, utilizes a messaging system called Location Discovery Message (LDM), that gets periodically sent by switches in the system out on all of their ports. This protocol enables loop free propogation of packets by ensuring that packets travel down the topology and not back up. ConclusionPortland is a data center network fabric that utilizes PMACs, a Fabric Manager, and a Location Discovery Protocol as the core components for improving data handling in large scale data centers. This design improves on forwarding table sizes, which when layer 2 mapped, can contain 100,000 or more entries [1] Mysore, Pamboris, Farrington, Huang, Miri, Radhakrishnan, Subramanya, and Vahdat. 2009. PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric","categories":[],"tags":[{"name":"Data centers","slug":"Data-centers","permalink":"http://ralucas.github.io/tags/Data-centers/"},{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"PortLand","slug":"PortLand","permalink":"http://ralucas.github.io/tags/PortLand/"},{"name":"Ethernet","slug":"Ethernet","permalink":"http://ralucas.github.io/tags/Ethernet/"}],"keywords":[]},{"title":"Securing Internet Routing","slug":"Securing-Internet-Routing","date":"2016-04-23T06:00:00.000Z","updated":"2016-04-23T16:57:33.000Z","comments":true,"path":"2016/04/23/Securing-Internet-Routing/","link":"","permalink":"http://ralucas.github.io/2016/04/23/Securing-Internet-Routing/","excerpt":"","text":"Border Gateway Protocol (BGP)BGP, an important piece of the larger Internet’s architecture, is a protocol that enables the setting up of routing and exchange of reachability information between large networks (Autonomous Systems). It’s the protocol used to develop routes between these Autonomous Systems (ASes), routing to destinations via IP prefixes. BGP makes announcements that the ASes use to discover routes to these IP prefixes. BGP is important to the business of the Internet, advertising routes, routing based on business relationships. ASes usually make sure that it’s in their “best” interest before routing traffic and there is a rule-of-thumb that seems to be generally followed: An AS will advertise a route to a neighboring AS if: The neighbor is a customer, or The route is for a prefix originated by the advertising AS, or The route is through a customer of the advertising AS BGP was originally introduced, like many of the protocols in use in the Internet, in the early days, when the Internet was more innocent. So, issues with security persist surrounding the protocol. BGP Security RisksHijacksBGP doesn’t have in place any mechanism to properly authenticate allocations of IP prefixes to ASes, so as a result entire IP prefix blocks can be hijacked fairly easily, either accidently or purposefully. There are two main types of hijacks: Prefix and subprefix. Prefix:The hijacker AS originates and announces the exact same prefix as a legitimate AS that has the IP allocation. The announcement gets propogated through the system and other ASes begin chosing their routes for that IP prefix and some will chose to go through the bogus AS, others won’t, given route length. Subprefix:The hijacker AS can potentially intercept 100% of the network traffic. Here the hijacker originates a prefix that is covered by the victim IP prefix. BGP uses longest-prefix match (LPM), so if a hijacker were to advertise a prefix that was LPM, then traffic would redirect to the hijacker AS. Route LeaksThis isn’t a bogus route, but instead the leaker announces a legitimate route, but does it to too many of its neighbors. So, then the leaker is overwhelmed by traffic from its neighbors that are now utilizing the leaked route. This can be disastrous if the leaker is not designed to handle high levels of traffic. Path-Shortening AttackAn attacker announces a short bogus path to a prefix that terminates at an authorized origin AS. Protocol downgrade attackASes that have, for instance, deployed BGPSEC can be convinced by an attacker to select a bogus path instead of the secured route because it’s cheaper. Incident ImpactBlackhole:Network traffic stops at the perpetrator AS and goes no further. Interception:Perpetrator AS invisibly intercepts traffic and the traffic also continues onto it’s intended destination. BGP Defense MechanismsPrefix filteringA whitelisting technique used to filter out bad BGP announcements. It works by using the AS rule-of-thumb and keeping a prefix list of IP prefixes of customers and ignoring any announcement from a customer not on the list. This defense has been used since the 1990s. Upsides: It is simple and effective mechanism and if all ASes deployed it, a large portion of routing leaks and hijacks would be prevented. Downsides: The downside is that it only works on customer leaks. ASes also aren’t necessarily incentivized to deploy this filtering outside of good Internet citzenship. RPKIResource Public Key Infrastructure (RPKI) is another defense method that provides a trusted mapping from allocated IP prefixes to BGP authorized ASes for origination. It creates a cryptographic hierarchy of authorities, which is rooted at the regional Internet registries (i.e. ARIN, RIPE, AfriNIC, etc.). The holder of the certificate for a prefix can then sign an authorization allowing a prefix to be originated via BGP. Upsides: Does not require any modification to current BGP message formats Cryptography can be performed offline As opposed to Prefix Filtering, it doesn’t tie itself to potential political/business conflict of interests as it can be used to filter BGP announcements made by any neighbor. Downsides: Potential for abuse of the RPKI, i.e. RPKI being attacked, misconfigured, or abused in some other way in which trust is lost in the protocol. Cannot prevent route leaking attacks as it’s purpose is to prevent unauthorized messages, whereas route leaks come from authorized origins. Cannot prevent path-shortening attack as the origin is legitimate and shortest path takes precedence. BGPSECBGPSEC, currently in standardization process by the IETF, builds on RPKI, adding crypto signatures to BCP messages. Each AS must sign a BGP message upon announcement. The signature includes the prefix and AS-level path, the AS number of the AS receiving the message, and all the previous signed messages received from previous ASes on the path. Upsides: No path-shortening attacks are possible because a shortened path would not pass the signature checks required as the origin would be checked against neighbor signatures. Downsides: Unlike RPKI, BGPSEC is online crypto as routers sign and verify the BGP messages. This has a higher computational load, requiring the routers to be designed and built with that in mind. To gain the full benefits of BGPSEC, every AS needs to deploy it. This requires that the already decentralized ASes, who have their own political and business objectives, to agree to use this protocol. To remedy this, one way is to gain traction via early adoption by some of the ASes ASes tend to prioritize economic demands over those of security demands and given that BGPSEC only provides some small benefits over RPKI, ASes are not as incentivized. Suffers from protocol downgrade attacks. [1] Goldberg, 2014. Why is it taking so long to secure internet routing","categories":[],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"BGP","slug":"BGP","permalink":"http://ralucas.github.io/tags/BGP/"},{"name":"Security","slug":"Security","permalink":"http://ralucas.github.io/tags/Security/"},{"name":"Routing","slug":"Routing","permalink":"http://ralucas.github.io/tags/Routing/"}],"keywords":[]},{"title":"Jellyfish Data Center Topology Review","slug":"Jellyfish-Data-Center-Topology-Review","date":"2016-04-22T06:00:00.000Z","updated":"2016-04-27T09:02:57.000Z","comments":true,"path":"2016/04/22/Jellyfish-Data-Center-Topology-Review/","link":"","permalink":"http://ralucas.github.io/2016/04/22/Jellyfish-Data-Center-Topology-Review/","excerpt":"","text":"What is Jellyfish?Jellyfish, an incrementally-expandable, high-bandwidth data center networking topology based on randomness. Definition: a degree-bounded random graph topology among top-of-rack switches. Basically, it’s an unstructured network that utilizes a randomized topology for interconnection in a data center, unlike, for example a fat tree topology that utilizes hierarchies (Figure 1). It allows for construction of arbitrary-size networks that can be easily incrementally expanded as needs arise in the data center for capacity or bandwidth. Jellyfish challengesAs a result of an unstructured data center network, challenges arise: Routing Physical construction Cabling layout How it’s constructedUsing the assumption that every switch has the same number of ports and servers, then with N racks, the network supports N(k - r) servers, where k is the number of ports and r is the number of ports used to connect to other racks. So, k - r is the number of ports that can be used to connect to servers. To wire this up, just pick a random pair of switches with free ports and link them and continue to do this process until all links are exhausted. Key: Path LengthAccording to the authors, the end-to-end throughput of a data center’s topology (how fast does data flow through it) isn’t solely dependent on the capacity (bandwidth/speed) of the network. Another important metric is the amount of network capacity that’s consumed in delivering each byte. This is represented in average path length. In order to deliver each byte of data, a network takes a series of hops in delivering that data. The less hops, the lower the average path length, the less network capacity consumed, therefore the faster the data can flow. Jellyfish has a lower average path length than standard hierarchal models. (a) Fat-tree path lengths (b) Jellyfish path lengths Key: Flexibility and ExpansionJellyfish’s construction is such that incrementally adding even just one server rack or switch is quite simple. The only rewiring necessary is limited to the number of ports being added. Additionally, Jellyfish allows for expansion using newer equipment that may have higher port-counts. One note here: Expansion may not produce uniform-random graph. However, topologies built incrementally versus those built from scratch show similar capacity throughputs and path lengths. Key: ResilienceJellyfish, in the face of failures, still maintains it’s structure. Additionally, Jellyfish, in comparison with a fat-tree topology with less servers, supports the same capacity, path-length, and resilience. CablingIn Jellyfish, there are more than twice as many cables running between switches than from servers to switches, so placing all the switches closely together makes sense. It’s suggested that the switch-cluster, where the majority of cables congregate, be placed in the center of the data center, with aggregate cable bundles running to each server-rack. Space should be left in the middle for additional switches and then more servers will just be added to the outside. In massive data centers, Jellyfish can be adapted with all of the pluses that it offers over fat-tree. ConclusionJellyfish, a random graph based network topology, is a scalable topology for data centers that makes setup and expandability simple. Jellyfish, also, can support 25% more servers than fat-tree, has on-average shorter path-length, is highly failure resilient, and shows network capacity of better than 90% of other known topologies.","categories":[],"tags":[{"name":"Jellyfish","slug":"Jellyfish","permalink":"http://ralucas.github.io/tags/Jellyfish/"},{"name":"Data centers","slug":"Data-centers","permalink":"http://ralucas.github.io/tags/Data-centers/"},{"name":"Scaling","slug":"Scaling","permalink":"http://ralucas.github.io/tags/Scaling/"},{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"}],"keywords":[]},{"title":"Understanding DASH (Dynamic Adaptive Streaming over HTTP)","slug":"Understanding-Dynamic-Adaptive-Streaming-over-HTTP","date":"2016-04-20T06:00:00.000Z","updated":"2016-04-20T07:08:08.000Z","comments":true,"path":"2016/04/20/Understanding-Dynamic-Adaptive-Streaming-over-HTTP/","link":"","permalink":"http://ralucas.github.io/2016/04/20/Understanding-Dynamic-Adaptive-Streaming-over-HTTP/","excerpt":"","text":"What is DASH, comparatively?Dynamic Adaptive Streaming over HTTP (DASH) is a standard that intends to address weaknesses in RTSP and progressive download methods currently used in streaming content over the internet. RTSPRTSP is a streaming protocol that manages a stateful connection with the client, tracking the state through the client interation. ProgressiveProgressive download can utilize HTTP and uses byte range requests to respond to the client. The issues with progressive are that it’s not bitrate responsive, bandwidth can potentially be wasted if the user switches their request, and doesn’t do live media. Despite these weakeness, progressive download enjoys wide adoption Why use HTTP?Content Delivery Networks (CDNs) have proliferated all over the world and given this, their scale, and their reliability, using them to manage streaming services just makes sense and…they use HTTP. Here are the nine reason’s Mr. Stockhammer gives [1] (my paraphrasing): HTTP Streaming is spreading widely as a form of delivery of Internet Video Trend towards HTTP as the main protocol for media delivery over the Internet Given the wide deployment of HTTP along with the TCP/IP stack, it probides reliabiliy and easy deployment HTTP avoids NAT and firewall traversal issues as it doesn’t require a dedicated connection…it’s stateless Allows delivery mechanisms and common methods such as CDNs, HTTP Caches, and HTTP Servers to be used, which already exist in profligate Moves control of the HTTP session to the client Provides the ability of the client to choose content rate given bandwidth as opposed to having the streaming server negotiate that Allows content rate to be seamlessly changed on the fly given changes in bandwidth Has the potential to accelerate fixed-mobile convergence. DASH GoalsThe DASH solution is intended to: support delivery of media in ISO base media file formats stay out of presentation logic permit integration of different presentation frameworks The protocol then in turn intends to define: Media Presentation as a stuctured collection of data Formats of Segments, an integral data unit of a Media Presentation, that can be HTTP URI addressable Delivery protocol of Segment, i.e. HTTP/1.1 A how-to for the client to utitlize the above information to establish a streaming service It supports features: Fast initial startup and seeking bandwidth efficiency adaptive bitrate switching adaption to CDN properties reuse of HTTP-servers and caches reuse of existing media playout engines Support for on-demand, live play Simplicity OverallDASH is a media streaming protocol that seeks to improve upon some of the known issues with RTSP and progressive download, while taking advantage of an already common and well-supported application protocol, HTTP. It was intended to be easily adapted to existing infrastructure, particularly the usage of HTTP CDNs. It’s intended to be generic enough so that it’s independent of media format. It’s main features are it’s adaptatability to changing network bandwidth, existing HTTP infrastructure, media formats, and flexibility to client presentations. [1] Stockhammer, 2011. Dynamic Adaptive Streaming over HTTP - Design Principles and Standards[2] Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update, 2015–2020 White Paper, February 2016","categories":[],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"HTTP","slug":"HTTP","permalink":"http://ralucas.github.io/tags/HTTP/"},{"name":"DASH","slug":"DASH","permalink":"http://ralucas.github.io/tags/DASH/"},{"name":"Streaming","slug":"Streaming","permalink":"http://ralucas.github.io/tags/Streaming/"}],"keywords":[]},{"title":"Understanding Controlling Queue Delay and CoDel","slug":"Understanding-Controlling-Queue-Delay","date":"2016-04-19T06:00:00.000Z","updated":"2016-04-20T04:25:28.000Z","comments":true,"path":"2016/04/19/Understanding-Controlling-Queue-Delay/","link":"","permalink":"http://ralucas.github.io/2016/04/19/Understanding-Controlling-Queue-Delay/","excerpt":"","text":"This is a survey of the “Controlling Queue Delaty” academic paper by Kathleen Nichols and Van Jacobson from ACMQueue in 2012.[1] Bufferbloat is still a problem!Packet-based networks use buffers to handle short-term fluctuation in traffic flow. Bufferbloat describes the standing packet queue that’s created when there is a mismatch between the senders maximum send rate (window size) and the bottleneck’s pipe size. This issue of bufferbloat has been known for many decades now. And given all the streaming today, it’s an important issue as applications today are much more delay-sensitive. The Standing QueueThis standing queue is the result from this mismatch. Because it creates delays and doesn’t improve throughput, it gets seen as a congestion issue and not properly seen in the light of queue or traffic theory. This is due to theorists seeing TCP’s reliable and highly correlated process, in which the packet arrival rate equals the departure rate, as improbable. Not an easy problemAnd the problem has been difficult to solve. Given that senders set the window size, queues begin to build up at the bottlenecks. This, thereby, makes it difficult for senders to adjust their sending rate given the constantly changing with conditions (flows coming and going, physical conditions, etc.) that occur in the bandwidth of the bottlenecks. Reliable Detection is HardA large queue is necessary to get started as the buffer needs to initially fill, so queue size really doesn’t give us much information about excess queue. A good queue is occupancy that goes away in about one RTT; bad queue persists for several RTTs [1] Active Queue Management (AQM)Active Queue Management is the solution for this bufferbloat problem, but it’s lack of widespread adoption has emanated from implementation difficulties and misunderstanding in the overall issue. AQM Goals Paramaterless - no implementation needed Treats good and bad queue differently Controls delay Adapts to dynamically changing link rates Simple and efficient - from home to enterprise use CoDel: A proposed solutionCoDel (Contolled Delay Management) is a proposed piece to that solution to this issue of queue management. It makes three main innovations to it’s algorithm: Using the local minimum queue to measure standing queue and not based on any of the previously used metrics such as the average size, etc. Keeping a single-state variable measuring how long the minimum have above or below the target for standing queue rather than a window of values to compute the minimum. Using the packet-sojourn time through the queue as the unit of measure as opposed to bytes or packets. Why the minimum value?The minimum packet sojourn can be decreased only when a packet is dequeued, so CoDel can go to work when packets are dequeued for sending and no locks are needed in the software. If the buffer is full on arrival, then a packet is dropped. The target and intervalThe target represents the “acceptable standing queue delay.” Very importantly, it’s measured in packet-sojourn time, not bytes or packets. Additionally, it’s unacceptable to drop packets if there are fewer than one MTU of bytes in the buffer.The interval represents “a time on the order of a worst-case RTT of connections through the bottleneck.” Let’s put it togetherSo, the crux here is that when the queue delay (it’s packet-sojourn time) is more than the target for at least interval time, then a packet is dropped and it’s not until the queue delay gets itself back below target that it stops dropping packets. In it’s dropping mode, it’s ‘next drop time’ is decreased each time as 1/SqRt(total_number_of_drops) with the intention of getting a linear change in throughput (TCP flow rate). ConclusionCoDel looks to be the best way to attack queue delay. While not the silver bullet for queue delay, it satisfies the five goals it set out. Manufacturers still need to build and market devices with proper buffer management. Further ReadingCoDel IETF DraftCoDel Wiki [1] Nichols and Jacobsen, 2012. Controlling Queue Delay","categories":[],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"TCP","slug":"TCP","permalink":"http://ralucas.github.io/tags/TCP/"},{"name":"Router buffers","slug":"Router-buffers","permalink":"http://ralucas.github.io/tags/Router-buffers/"},{"name":"Bufferbloat","slug":"Bufferbloat","permalink":"http://ralucas.github.io/tags/Bufferbloat/"},{"name":"CoDel","slug":"CoDel","permalink":"http://ralucas.github.io/tags/CoDel/"},{"name":"Queue delay","slug":"Queue-delay","permalink":"http://ralucas.github.io/tags/Queue-delay/"}],"keywords":[]},{"title":"Understanding Sizing Router Buffers","slug":"Understanding-Sizing-Router-Buffers","date":"2016-04-17T06:00:00.000Z","updated":"2016-04-20T04:22:36.000Z","comments":true,"path":"2016/04/17/Understanding-Sizing-Router-Buffers/","link":"","permalink":"http://ralucas.github.io/2016/04/17/Understanding-Sizing-Router-Buffers/","excerpt":"","text":"This is a survey of the “Sizing Routing Buffers” academic paper by Appenzeller, Keslassy, and McKeown published in 2004. In the past, Internet routers had buffers that used a rule-of-thumb using the equation of: B = RTT X C. Where RTT is the round-trip time of a packet and C is the data rate. This rule-of-thumb is no longer useful. The primary goal of the rule-of-thumb, or any for that matter, is to keep a link as close to 100% utilization as possible, as this maximizes the throughput of the network. Why is it no longer useful? Large buffers are difficult to manufacture, given that they must use slow, off-chip DRAM for memory buffering. DRAM (dynamic RAM) has some problems. It has an access time of about 50ns, whereas a packet of size 40bytes, can arrive and depart in 8ns: DRAM is slow and slows network speed down. Additionally, they require utilization of a wide DRAM bus, which requires a large number of data pins, which in turn increases the power consumption of the DRAM boards. Large buffers can contribute to lengthening queueing delays which can in turn cause problems with congestion control algorithms, as they rely on packet loss in order to inform them of TCP congestion. The original rule-of-thumb came from a 1994 paper and experiments that used a small number of multiplexed flows (up to 8) on a now comparatively low-speed connection of only 40 Mb/s. Today, for example, typical backbone links are now beginning to operate at 100 Gb/s or greater[2] with many thousand multiplexed flows. Some consider router buffers to be one of the largest contributors to network traffic speed uncertainty. Why is Overbuffering bad? In order to accomodate large buffers, routers have to be designed accordingly, leading to larger power consumption, more board space, and lower density. It increases delay when congestion occurs and conflict with low-latency real-time applications like video games. And in some cases, make applications completely unusable. One of the main issues with the idea that a buffer should be utilized to maximize the network throughput is that it doesn’t take into account TCP Congestion Control algorithms that rely on a packet loss to indicate congestion and to go into action. TCP will always make the buffer overflow in order to lead to packet loss. Single-flow and Synchronized flowsAccording to the paper, “the key to sizing the buffer is to make sure that the buffer is large enough, so that while the sender pauses, the buffer doesn’t go empty.” In the case of the single, long-lived flow and synchronized TCP flows, for the buffer to never go empty it needs to be half of the window size max. This is because of how TCP performs. When it TCP sees packet loss, it goes into AIMD congestion control mode and drops it’s packet flow by half. So, while the buffer almost hits zero, it never quite does. Desynchronized flowsMost internet backbones handle enough simultaneous flows, i.e. greater than 500, that they usually operate under a desynchronized flow pattern. In this environment, the TCP sawtooth isn’t synched, so the more flows that are added, the window size from peak to trough is smoothed out and gets smaller. So, given that as discussed before, that the amount of buffer needed to maintain the network utilization was half of the window max size, then if the window size is now smaller, then therefore the buffer size can be even smaller. Also, in this case, the flow length distribution is heavily distributed towards long flows, as they typically take up 80% of the bandwidth. However, most of the flows are in fact from short-lived flows (fewer than 100 packets). But, they only account for about 20% of the bandwidth. This is partly due because short flows usually don’t get out of TCP slow-start mode and never fully utilize the network bandwidth, whereas long flows typically get into congestion control mode, maximizing the network bandwidth. ConclusionsThrough this survey, it is apparent that smaller buffer sizes are imperative for best network bandwidth utilization no matter the type of flow or flow environment. Smaller buffers increase bandwidth utilization because they 1) signal to the TCP flow when to go into congestion mode sooner, 2) larger buffers can lead to delay, and 3) are cheaper to manufacture. [1] Appenzaller, Keslassy, McKeown, 2004. Sizing Routing Buffers[2] Malik, 2013. 100G, 200G, 400G: Internet’s core is getting fatter to meet our tech planet’s bandwidth demand","categories":[],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://ralucas.github.io/tags/Networking/"},{"name":"TCP","slug":"TCP","permalink":"http://ralucas.github.io/tags/TCP/"},{"name":"Router buffers","slug":"Router-buffers","permalink":"http://ralucas.github.io/tags/Router-buffers/"}],"keywords":[]},{"title":"Useful CLI commands","slug":"Useful-CLI-Commands","date":"2015-08-15T03:37:59.000Z","updated":"2016-04-27T13:41:11.000Z","comments":true,"path":"2015/08/14/Useful-CLI-Commands/","link":"","permalink":"http://ralucas.github.io/2015/08/14/Useful-CLI-Commands/","excerpt":"","text":"Some CLI Commands that I’ve found useful:Viewing processes and killing them$ lsof -Pi | grep LISTEN Displays processes running and ports The PID is the second number (2nd column from left) $ kill -9 $PID This kills the process at the entered PID with extreme prejudice Using find$ find . -name &#39;*.css&#39; This will recurse all directories and list all CSS files (and directories ending with “.css”) under the current directory (represented by “.”) $ find . -type f -name &#39;*.css&#39; This will only match CSS files (case-sensitively) $ find . -name &quot;*.css&quot; -exec grep -l &quot;#content&quot; {} \\; This example finds all CSS files that do something with your HTML ID #content $ find . -mtime -1 -type f find files changed in the last 1 day $ find . \\! -path &quot;*CVS*&quot; -type f -name &quot;*.css&quot; find CSS files, omitting results containing “CVS” $ find ~/src -newer main.css find files newer than main.css in ~/src $ find . -name \\*.css -print0 | xargs -0 grep -nH foo combine with xargs for more power than -exec Deleting a folder and it’s contentsPlease use carefully $ rm -rf [folder_name] Always include a folder name Copying a folder and contents$ cp -avr /from/folder /to/new_folder -a : Preserve the specified attributes such as directory an file mode, ownership, timestamps, if possible additional attributes: context, links, xattr, all. -v : Explain what is being done (verbose). -r : Copy directories recursively. What is grep It is a command that finds text/strings inside of files It can take regular expressions recurse through a directory $ grep -r &#39;lookforthisexpression&#39; files/* case insensitive search $ grep -i &#39;anycaseterm&#39; file.txt Make a file (…for testing…) mkfile command input number, then size (i.e. k for kb or g for gb), then filename $ mkfile -n [size][b|k|m|g] [filename] Heroku commands Get to the console/bash on the remote server $ heroku run bash --app [put_app_name_here] Compiling bash scripts$ chmod +x ./[bash_script.sh] Redirect to port 80 on linux server$ sudo iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 3000","categories":[],"tags":[{"name":"CLI","slug":"CLI","permalink":"http://ralucas.github.io/tags/CLI/"},{"name":"Linux","slug":"Linux","permalink":"http://ralucas.github.io/tags/Linux/"}],"keywords":[]},{"title":"Resizing image with imagemagick","slug":"Resizing-image-with-imagemagick","date":"2015-08-12T03:46:31.000Z","updated":"2015-08-12T03:46:31.000Z","comments":true,"path":"2015/08/11/Resizing-image-with-imagemagick/","link":"","permalink":"http://ralucas.github.io/2015/08/11/Resizing-image-with-imagemagick/","excerpt":"","text":"Quick one-linerUsing imagemagick to resize an image, keep it’s proportion (i.e. 4 x 3 ), and place it in the middle with a white background: 1convert input.png -resize 1280x800 -background white -gravity center -extent 1280x800 output.png","categories":[],"tags":[{"name":"imagemagick","slug":"imagemagick","permalink":"http://ralucas.github.io/tags/imagemagick/"},{"name":"resizing images","slug":"resizing-images","permalink":"http://ralucas.github.io/tags/resizing-images/"}],"keywords":[]},{"title":"Mocking out the Request module in Node.js for Testing","slug":"Mocking-out-the-request-module-for-testing","date":"2015-08-11T05:09:26.000Z","updated":"2015-08-11T05:09:26.000Z","comments":true,"path":"2015/08/10/Mocking-out-the-request-module-for-testing/","link":"","permalink":"http://ralucas.github.io/2015/08/10/Mocking-out-the-request-module-for-testing/","excerpt":"","text":"So, recently I needed to write a test that required that I mock the request module out as I wanted to pass a specific error code back to the response handler and do something different with it. In this case, I wanted to retry the request. I first attacked the issue with Sinon.js library. If you’ve ever tested in javascript, sinon.js is an indispensable mocking and spying library. So, I wrote: 1234567891011...var request = require(&apos;request&apos;),...before(done) &#123; stub = sinon.stub(request) .withArgs(&#123; method: &apos;GET&apos;, url: &apos;http://example.com&apos; &#125;); done();&#125; However, that doesn’t work as sinon.js requires if you’re stubbing out a module for there to be method that you use, i.e. stub = sinon.stub(request, &#39;get&#39;) … see this StackOverflow issue The problem was that the api that I was testing had already largely been written and I didn’t now want to rewrite a bunch of it just to make it testable for this one case. So, enter Mockery. Mockery is great in this instance, but be careful when you use it as it tears down the require statement, so use it as tightly as you can and tear it down immediately after use. So, I separated out my test: myModule.js: 123456789101112131415161718...var request = require(&apos;request&apos;);...function handler(callback) &#123; return function(err, res, body) &#123; if (err) &#123; //doStuff &#125; ...//doOtherStuff &#125;&#125;exports.getSomeStuff = function(url, callback) &#123; return request(&#123; method: &apos;GET&apos;, url: url &#125;, handler(callback));&#125;; test.js: 1234567891011121314151617181920212223242526272829303132333435363738394041424344...var request = require(&apos;request&apos;);... describe(&apos;request error testing&apos;, function() &#123; var stub, module; before(function(done) &#123; mockery.enable(&#123; warnOnReplace: false, warnOnUnregistered: false, useCleanCache: true &#125;); stub = sinon.stub(); mockery.registerMock(&apos;request&apos;, stub); mm = require(&apos;./myModule&apos;); done(); &#125;); after(function(done) &#123; mockery.disable(); done(); &#125;); it(&apos;should retry once on ETIMEDOUT error&apos;, function(done) &#123; var spy = sinon.spy(client, &apos;request&apos;); var err = new Error(&apos;connect ETIMEDOUT&apos;); stub.yields(err, null, null); mm.getSomeStuff(&apos;http://example.com&apos;, function(err, json) &#123; expect(spy.calledOnce).to.be.true; expect(stub.calledTwice).to.be.true; expect(err.message).to.include(&apos;ETIMEDOUT&apos;); done(); &#125;); &#125;); &#125;); So, there you have it. Comment if you have any thoughts or questions. Another really great post that helped me: http://bulkan-evcimen.com/using_mockery_to_mock_modules_nodejs.html Cheers! Richard","categories":[],"tags":[{"name":"Testing","slug":"Testing","permalink":"http://ralucas.github.io/tags/Testing/"},{"name":"Node.js","slug":"Node-js","permalink":"http://ralucas.github.io/tags/Node-js/"},{"name":"Request","slug":"Request","permalink":"http://ralucas.github.io/tags/Request/"},{"name":"Mockery","slug":"Mockery","permalink":"http://ralucas.github.io/tags/Mockery/"},{"name":"Sinon.js","slug":"Sinon-js","permalink":"http://ralucas.github.io/tags/Sinon-js/"}],"keywords":[]},{"title":"Importing CSV in Postgres","slug":"Importing-CSV-in-Postgres","date":"2013-12-11T01:19:18.000Z","updated":"2015-02-02T01:19:50.000Z","comments":true,"path":"2013/12/10/Importing-CSV-in-Postgres/","link":"","permalink":"http://ralucas.github.io/2013/12/10/Importing-CSV-in-Postgres/","excerpt":"","text":"Log into the database from the terminal:1$ psql database_name Create a new table:123456789# CREATE TABLE table_name(column_name data_type, column_name2 data_type, etc);example:# CREATE TABLE trips(block_id int,route_id text,direction_id int,trip_headsign text,shape_id text,service_id text,trip_id text);//And then you should see:CREATE TABLE Copy the data from a CSV:12345678# COPY table_name FROM &apos;/path/to/CSV_file.txt&apos; DELIMITER &apos;,&apos; CSV;example:# COPY trips FROM &apos;/home/username/Downloads/data/trips.txt&apos; DELIMITER &apos;,&apos; CSV;//And then you should see:COPY XXXX &lt;-number of lines View the tables and select the table you wish to view:12# \\dt# SELECT * FROM table_name; Cheers!","categories":[],"tags":[{"name":"Postgres","slug":"Postgres","permalink":"http://ralucas.github.io/tags/Postgres/"},{"name":"CSV","slug":"CSV","permalink":"http://ralucas.github.io/tags/CSV/"}],"keywords":[]},{"title":"Installing Postgres on Linux Mint","slug":"Installing-Postgres-on-Linux-Mint","date":"2013-12-02T01:10:59.000Z","updated":"2015-02-02T03:25:55.000Z","comments":true,"path":"2013/12/01/Installing-Postgres-on-Linux-Mint/","link":"","permalink":"http://ralucas.github.io/2013/12/01/Installing-Postgres-on-Linux-Mint/","excerpt":"","text":"###A Quick Guide Open up the terminal:1$ sudo apt-get install postgresql Installs Postgresql. This will take a few minutes. Next:1$ sudo su - postgres This logs you into the database as the postgres user. You will then want to create yourself as a user:1$ createuser your_username Now, at the command prompt under your username, you are able to use createdb and other commands. So, let’s create a quick database and connect to it.1$ createdb myFirstDb Connect to it:1$ psql myFirstDb You will then see a prompt that will look like:1myFirstDb=# The # indicates that you are the superuser, which is most likely if you just installed postgres on your local computer. Let’s run:1myFirstDb=# SELECT Version(); This should go into a new screen on your terminal that should look something like this:123456 version ------------------------------------------------------------------------------------------------------- PostgreSQL 9.1.10 on i686-pc-linux-gnu, compiled by gcc (Ubuntu/Linaro 4.8.1-10ubuntu7) 4.8.1, 32-bit(1 row)(END) Type q to exit the screen and go back to the # prompt. You can run commands from the # prompt using \\ notation and the official documentation on commands can be found here: http://www.postgresql.org/docs/9.3/interactive/app-psql.html If you want a GUI client:1$ sudo apt-get install pgadmin3","categories":[],"tags":[{"name":"Postgres","slug":"Postgres","permalink":"http://ralucas.github.io/tags/Postgres/"},{"name":"Linux Mint","slug":"Linux-Mint","permalink":"http://ralucas.github.io/tags/Linux-Mint/"}],"keywords":[]},{"title":"Using Crockford's supplant function in Javascript","slug":"Using-Crockford-s-supplant-function-in-Javascript","date":"2013-11-02T00:09:07.000Z","updated":"2015-02-02T01:10:14.000Z","comments":true,"path":"2013/11/01/Using-Crockford-s-supplant-function-in-Javascript/","link":"","permalink":"http://ralucas.github.io/2013/11/01/Using-Crockford-s-supplant-function-in-Javascript/","excerpt":"","text":"The supplant function is easy to use for string interpolation and a nice introduction templating in JavaScript. Here’s the function from Douglas Crockford’s website: 123456789if (!String.prototype.supplant) &#123; String.prototype.supplant = function (o) &#123; return this.replace(/\\&#123;([^&#123;&#125;]*)\\&#125;/g, function (a, b) &#123; var r = o[b]; return typeof r === &apos;string&apos; || typeof r === &apos;number&apos; ? r : a; &#125;); &#125;;&#125; So how is it used. Here’s a quick example: var greeting = &quot;Hi, I&#39;m {name}&quot;.supplant[{name:&quot;Richard&quot;}]; Output: “Hi, I’m Richard” Another example: 1234567myObj = &#123; name: &apos;Richard&apos;, city: &apos;Columbus&apos;, state: &apos;Ohio&apos; &#125;; var myInfo = &quot;Hi, my name is &#123;name&#125; and I&apos;m from &#123;city&#125;, &#123;state&#125;&quot;.supplant(myObj); Output: “Hi, my name is Richard and I’m from Columbus, Ohio” You can use it with arrays: 123var names = [&quot;Richard&quot;, &quot;Yalcin&quot;, &quot;Dan&quot;, &quot;Mike&quot;, &quot;Kerry&quot;];var classmates = &quot;My classmates are &#123;0&#125;, &#123;1&#125;, &#123;2&#125;, &#123;3&#125;, and &#123;4&#125;&quot;.supplant(names); Output: “My classmates are Richard, Yalcin, Dan, Mike, and Kerry” These are a few examples, hopefully you do find them helpful. Supplant does take some criticism for being inefficient as well as unescapable, which are both founded. So, you may want to use supplant sparingly, but it’s certainly a great tool in learning JavaScript. Here’s a stack overflow post that discusses it further as well. And as you delve deeper into string interpolation and templating, I suggest looking into Handlebars.js or Mustache. As always, if you have any questions, please don’t hesitate to ask. Cheers!","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://ralucas.github.io/tags/JavaScript/"},{"name":"Supplant","slug":"Supplant","permalink":"http://ralucas.github.io/tags/Supplant/"},{"name":"Templating","slug":"Templating","permalink":"http://ralucas.github.io/tags/Templating/"}],"keywords":[]},{"title":"Late Fragment by Raymond Carver","slug":"Late-Fragment-by-Raymond-Carver","date":"2013-07-02T00:05:36.000Z","updated":"2015-02-02T01:43:40.000Z","comments":true,"path":"2013/07/01/Late-Fragment-by-Raymond-Carver/","link":"","permalink":"http://ralucas.github.io/2013/07/01/Late-Fragment-by-Raymond-Carver/","excerpt":"","text":"Great poem by one of my favorite writers. Late Fragmentby Raymond Carver And did you get whatyou wanted from this life, even so?I did.And what did you want?To call myself beloved, to feel myselfbeloved on the earth.","categories":[],"tags":[{"name":"Raymond Carver","slug":"Raymond-Carver","permalink":"http://ralucas.github.io/tags/Raymond-Carver/"},{"name":"Poem","slug":"Poem","permalink":"http://ralucas.github.io/tags/Poem/"}],"keywords":[]},{"title":"Cleveland Marathon Race Report","slug":"Cleveland-Marathon-Race-Report","date":"2013-06-02T00:06:57.000Z","updated":"2015-02-02T03:25:42.000Z","comments":true,"path":"2013/06/01/Cleveland-Marathon-Race-Report/","link":"","permalink":"http://ralucas.github.io/2013/06/01/Cleveland-Marathon-Race-Report/","excerpt":"","text":"This past Sunday, I ran in the Cleveland Marathon. I’d been training for the past 4-6 months for the race. So, how did it go? Well, I finished with a 3:43 time, so reasonably well. I beat my goal time of 3:45 by a couple of minutes, so overall I’m quite happy with the results. As I’ve mentioned earlier, I’ve been quite worried that my right foot was going to bother me the entire race, but after about 2 miles or so, it went away or I completely forgot about it. ####My race plan: Start out a little slow at a 9-minute pace for the first few miles and then begin to speed up. Try to speed up in the second half of the race for a negative split. Try to speed up after mile 16, in which the majority of the course would be heading down. For the most part I stuck to this plan. I ran easy in the beginning and sped up a little, hitting the half marathon point at 1:50. I then actually did speed up for the next split at 30K, doing a 8:17 split. I was on pace and I was feeling great. We were running through Rockefeller park, underneath the shade, and it was perfect. But, then I started to slow up, particularly around mile 22. This seemed to be the loneliest, hottest stretch of the race. It was around I-90, there weren’t too many folks around, no shade and it was a long uphill. That was a battle. Between miles 21 and 25, I was really struggling. My heart was working hard and I was really starting to feel it. At the mile 23 water stop, I actually did stop to walk through it taking both a Powerade and water. I walked for a minute or so, but knew I needed to get going. Those couple of miles between 23 and 25, as I could see the downtown in the distance were tough miles. I tried to gut it out, keeping a 9:15 pace going. When I reached one mile to go, I tried to give it all I had and really got going up to speed with about a half mile to go. And I made it with a good time. ####My nutrition plan: Clif Shot Blocks - eat one every two miles just before the water station and get some water to wash it down. Then switch to the Honey Stingers - I broke these up and put them in a little ziplock bag. I stuck to the plan. I took one package (6 pieces) of the Shot Blocks, so for the first twelve miles I did that and then switched over to the Honey Stingers for the second half. I think it worked for the most part as I didn’t have any noticeable sugar fall-outs in the race. I think for the next race I’m just going to do two packages of the Shot Blocks as I was a little uncertain about how much of the Honey Stingers I was getting in. The Shot Blocks are just nicely dosed, so they’re easier to manage. My time and splits: http://live.xacte.com/cleveland/?id=110&amp;tagcode=1458 ####Takeaways from the race: My training over the 4-6 months prior was worth all the work. In particular, I really believe the weekly speed/interval training that I did was key for me maintaining my pace. My nutrition was solid and I like the Clif Shots, so I think I may go to those only for a race. There’s still some room for improvement and I’m going to move up my new goal to 3:30: I’m going to move my training pace up by 30 seconds to 9:30 pace. I need to stay on track with my weekly speed work and look into potentially adding a second day per week of speed work. I need to find a way to train myself better both physically and mentally for those difficult miles: 21-25. Begin training for the 50-mile ultramarathon (My hypothesis is that by training for a longer race, I will be able to easily run a shorter one). Take the VO2 Max test","categories":[],"tags":[{"name":"Marathon","slug":"Marathon","permalink":"http://ralucas.github.io/tags/Marathon/"},{"name":"Cleveland","slug":"Cleveland","permalink":"http://ralucas.github.io/tags/Cleveland/"},{"name":"Race Report","slug":"Race-Report","permalink":"http://ralucas.github.io/tags/Race-Report/"},{"name":"Running","slug":"Running","permalink":"http://ralucas.github.io/tags/Running/"}],"keywords":[]},{"title":"How I've trained for a marathon","slug":"How-I-ve-trained-for-a-marathon","date":"2013-06-01T23:57:43.000Z","updated":"2015-02-02T01:07:24.000Z","comments":true,"path":"2013/06/01/How-I-ve-trained-for-a-marathon/","link":"","permalink":"http://ralucas.github.io/2013/06/01/How-I-ve-trained-for-a-marathon/","excerpt":"","text":"I began really back in the summer of 2012, when I started training for the Columbus half-marathon and then the Whitney climb. After I ran the half, I continued running, and in talking with a co-worker who had experience in marathoning, I began planning a winter marathon. My initial plan was to run either the Jackson, MS or Baton Rouge/Louisiana Marathon, which were both taking place in January of 2013, I believe. So, I found Hal Higdon’s site and began following one of his plans. The site has plans for any level and a lot of great advice. So, I started increasing my mileage, eventually doing a 22 mile run. This was all in November/December of 2012. It was cold. I can remember doing the 22-mile run, not feeling super great upon embarkation, but doing it anyhow. And I was only taking with me a water belt, with one gatorade and one water bottle, and a peanut butter and jelly sandwich. Not enough really. So, by the end of the run, I was dead tired, leg muscles cramping, and actually had to stop and walk and few times (really no big deal, but still not what I was hoping for). I was unprepared and probably pushed my body when I wasn’t feeling it. During this time, I did decide to join a local running group, which began it’s ‘Winter Session’ in January. This was fine as I was going to do a spring marathon anyways and my goal was (and still is) 3 marathons in 2013. Joining the Marathoners-in-training group here in Columbus, for me, has been really great. I’ve learned a ton about best training practices and regimens, proper nutrition, pacing, and I’ve met a bunch of kind, like-minded folks. So, how I’ve trained: I started back in January for a May race, so 4 solid months, and I had some decent fitness already. If you don’t, you may want to give yourself some more time and begin to build a stronger base. My target pace is 8:30, which I think is doable, given my results in the half-marathon at a better pace (8:20). Now that’s not significantly better than 8:30, but I’m hoping that given my training, I can hit it. Prior to this January, I used to run at a much faster pace, 9-minute or faster. Now all my runs are usually at around 10-minute pace as it was recommended that the rule of thumb for pacing is to run most of your practice runs at 60-90 seconds slower than your goal marathon pace. One reason, I believe, for going a little slower is to build up your base. I tried a variety of foods while running: Clif Shot Blocks, GU, Gatorade gels, Gummy Bears, Honey Stingers, Peanut Butter and Jelly Sandwich, Chews, Buddy Fruits, Powerbars, and any number of protein bars. So, my favorites for running are the Clif Shot Blocks (usually Strawberry flavor) and the Honey Stingers. The GU and Gummy bears did not sit well with me, and I think they’re just too much sugar, too quickly, even if I do try to offset with some water. I could probably drink more water with them, but I just really like the shot blocks and stingers. I’m still struggling on finding the right pair of shoes. The primary reason is my feet are flat and so I overpronate. I’ve been using/comparing the Asics Gel Kayano and the Nike Lunar Eclipse. The real discovery for me has unfortunately come at the end of training, as now the ball of my right foot has begun to ache with the Metarsalgia I thought that I had gotten through. You can read more about it here. So, there you go. Let me know if you have any suggestions or comments on how I could be doing this better. Thanks.","categories":[],"tags":[{"name":"Marathon","slug":"Marathon","permalink":"http://ralucas.github.io/tags/Marathon/"},{"name":"Cleveland","slug":"Cleveland","permalink":"http://ralucas.github.io/tags/Cleveland/"},{"name":"Running","slug":"Running","permalink":"http://ralucas.github.io/tags/Running/"}],"keywords":[]},{"title":"Running Pains","slug":"Running-Pains","date":"2013-05-14T18:00:00.000Z","updated":"2015-02-02T01:02:19.000Z","comments":true,"path":"2013/05/14/Running-Pains/","link":"","permalink":"http://ralucas.github.io/2013/05/14/Running-Pains/","excerpt":"","text":"I’m just going to preface this post by saying I’m not a doctor, so this is simply my own experience. If you’re feeling foot pain you should see a doctor. So, over the past year or so, since I’ve really started running in earnest and outside a lot more, I’ve developed an intermittent pain in the ball of my foot. Last year, it was on my left foot, but now it has migrated to my right foot. When it happened last year, I got worried that maybe it was a stress fracture, so I went to the podiatrist right away. He gave the foot an X-ray and it wasn’t fractured. He told me, he thought it was a neuroma or metatarsalgia. And suggested, given my flat feet, I really should get custom orthotics. I still haven’t done that. Nonetheless, I took some time off, began running more, and the pain really ended up going away. It didn’t come back until I decided, being the smart guy that I am, that I needed to break in a new pair of running shoes for the upcoming marathon. And so on about the 3rd or 4th run, I started getting that dull, ball of the foot pain around the 2nd and 3rd toe. A real bummer. And now as I’m getting ready to run here this coming Sunday, my foot is still bothering a bit. Hopefully, I’ll be fine for most of the race and I’m going to just tough it out if it does bother me. It’s really not bad, just sort of a bothersome ache/pain. Here’s what I’ve learned, but don’t hold me to this: Need to make sure the toe box in the shoe is good and roomy. I’m going to try a new tying method. A new pair of shoes can potentially cause re-inflammation. Don’t break in a pair of shoes with a month to go in training. I actually thought I’d be fine. Lesson learned. Start out with a couple (or more) pairs. I probably should have done a bit more cross-training near the end when my foot was bothering me and I’m going to try that this summer, should it happen again.","categories":[],"tags":[{"name":"Running","slug":"Running","permalink":"http://ralucas.github.io/tags/Running/"},{"name":"Injury","slug":"Injury","permalink":"http://ralucas.github.io/tags/Injury/"}],"keywords":[]},{"title":"Tough Mudder Review","slug":"Tough-Mudder-Review","date":"2013-05-01T23:46:15.000Z","updated":"2015-02-02T03:24:57.000Z","comments":true,"path":"2013/05/01/Tough-Mudder-Review/","link":"","permalink":"http://ralucas.github.io/2013/05/01/Tough-Mudder-Review/","excerpt":"","text":"So, I did the Tough Mudder here in Ohio this past Saturday with my sister and a couple of people that joined our team after I put up forum post for team members. It was held in Mansfield, OH about an hour north of Columbus. My sister and I signed up for it, not really knowing much about it, back in the fall of last year. I had heard a friend mention it and I thought that it sounded like a good challenge. It costs us, I believe, around $120 to do the event. The Tough Mudder is basically an obstacles course with muddy terrain and is part of the growing phenomenon of these obstacle-type runs that seem to be popping up everywhere. It took place at what seemed to be a farm near Mansfield, Ohio. It’s a ten mile course and they had 22 obstacles. They range from crawling under barb wire for 15 feet to climbing over walls that are 15 feet to just running/walking through a muddy path. Some of the obstacles can be a bit tough, particularly as the day wears on. My sister and I signed up, but we needed to get more team members. Unfortunately, we couldn’t convince any of our friends to join us, so we put up a post on the forums and Adriana and Bob responded to join our team. We really couldn’t have asked for better teammates. They were great. Okay, so we trained and then finally came the day of the event. The emails from the organization told us to try to arrive 2 hours earlier than your start time. Being an unbeliever in showing up too early for anything, we blew that off and decided to show up an hour early. Well, we should have listened to them. There was a huge crowd, traffic, parking lines, and then we got bussed to the race field. We estimated that if they had about 9,000 participants per day going through the course. That’s quite a bit. We ended up arriving about 40 minutes late. So, we should have taken that 2 hours prior a bit more seriously. We really lucked out, though: it was a beautiful day - mid-60s and sunny. Upon arriving, we located Bob and Adriana and jetted off to start of the race. Before beginning, the MC gave a 20 minute speech, trying to rile everyone up, going over the standards and rules, and of course, not forgetting to mention all of the corporate sponsors. And then finally, we were off. The total time we spent out on the course was about 4-1/2 hours, which is basically the equivalent of a marathon time-wise. For the most part, we tried to run/jog between the obstacles, giving into a walk when the mud became particularly heavy. And the mud was heavy for the most part and it was everywhere. Tying your shoes on tight is essential if you want to keep them on. Bob was the only team member to have successfully completed all the obstacles. I felt like I did fairly well, only being unable to get past the monkey bars (my excuse: hands were too wet and slipped). Adriana completed the monkey bars successfully and was told that she was one of the few women to have accomplished that. And my sister had a lot of success and attempted every obstacle. Toughest obstacle: Berlin Walls - Part of the reason was that I initially attempted this solo and didn’t make it. Fortunately some fellows helped me over, but it was still rough pulling myself over the walls. As we neared the last 5 or 6 obstacles, lines began to grow and getting through the obstacles began to take more and more time. This was a bit of a detractor and I thought that perhaps they should begin to limit the amount of people on the course per day as it really seemed like too many. I will say this, I was sore for about two days after it and even now have large bruises under my arms from lifting myself up on the walls. In terms of training, the Tough Mudder website has some training suggestions, but I didn’t really attempt them. I felt that they were too difficult and crazy to attempt at the gym. My training was basically train for a marathon and I also did some weightlifting, swimming, and some of the DVD Insanity. So, I would do a workout of either weights, swim, or DVD in the morning and then do my mileage of running in the evenings. This is probably more than most people would have time to do, so I think that if you’re just maintaining a healthy running schedule with some weight training thrown in there, you would be fine and no need to workout twice a day, either. My conclusion: We had a fun time out there, got great weather, and met a couple of good folks. I don’t think I would do it again because, well, I guess I don’t see what the big deal about it is. Surely, it taxed me physically, but I didn’t feel like it was truly an intense physical experience. Certain obstacles were difficult, but the majority of them were very reasonable for anyone in decent shape to complete. If I was more flexible, the wall climbing may have been easier. I think, for myself, I didn’t really get that feeling of accomplishment and perhaps this was because I’ve had my sights on the completion of an upcoming marathon and then some mountain hiking. My recommendation would be, definitely do it if you feel like this would be something that you would enjoy and you’ve got a good group to do it with, but if you’re doing it to prove something to yourself, I think there are more difficult physical challenges out there. Thanks for reading this.","categories":[],"tags":[{"name":"Running","slug":"Running","permalink":"http://ralucas.github.io/tags/Running/"},{"name":"Tough Mudder","slug":"Tough-Mudder","permalink":"http://ralucas.github.io/tags/Tough-Mudder/"}],"keywords":[]},{"title":"Mt. Whitney via Mountaineer's Route","slug":"Mt-Whitney-via-Mountaineer-s-Route","date":"2012-10-01T23:37:15.000Z","updated":"2015-02-02T03:16:47.000Z","comments":true,"path":"2012/10/01/Mt-Whitney-via-Mountaineer-s-Route/","link":"","permalink":"http://ralucas.github.io/2012/10/01/Mt-Whitney-via-Mountaineer-s-Route/","excerpt":"Well, here I am on the summit of Mt. Whitney on a hike done in September 2012 via the Mountaineer’s Route. Unfortunately, given the oncoming clouds, we only had a minute or two to savor the accomplishment before we had begin the descent, but what a fun adventure! So, a little backstory here: In August, I had been dating a girl that I was beginning to like, and then suddenly she stopped calling me and only after probably too many texts on my part, she finally had the courage to send me the ol’ fashioned breakup text. Woe is me, right? I had to do something, so I looked online for a guided climb of Mt. Whitney (the tallest peak in the lower 48, oh yeah!). There didn’t seem to be one going around Labor Day when I was going to have a chance to take a trip, so I shot some guiding companies some emails. I contacted SWS Mountain Guides to see if I could get on a waitlist or if they had a guided hike going on around Labor Day (check them out here, they also do many other guided climbs and I think I’m going to do Shasta 2013 Summer - they were great, so I do recommend them). One of the owners responded and said that they were putting a group together for around that time. Next thing I know, he calls me, I give my credit card, and I’m signed up. “Shit!,” I thought, “now I’m going to Mt. Whitney, and I’ve never climbed a mountain, and really haven’t done too much backpacking. I’m just a guy from the suburbs in Columbus, Ohio. How is this going to go?” Fortunately, I had been doing some training for an upcoming half-marathon, so I wasn’t completely out of shape. But, still only 5 weeks to get ready. I stepped up my running and started backpacking with 30 lbs at the various parks around Columbus. I started to get a sharp pain in the ball of my foot from probably over-training. I suspect either Sesamoiditis or Metatarsalgia, but the podiatrist didn’t give me a diagnosis. Had an X-ray to eliminate whether or not it was a stress fracture and would it derail my planned trip (it wasn’t). Which would have been a real bummer, as I was now looking forward to it quite a bit. The doctor made up a custom little orthotic and I ended up just taking it a little easier and the pain began to subside. So, I was good for the trip.","text":"Well, here I am on the summit of Mt. Whitney on a hike done in September 2012 via the Mountaineer’s Route. Unfortunately, given the oncoming clouds, we only had a minute or two to savor the accomplishment before we had begin the descent, but what a fun adventure! So, a little backstory here: In August, I had been dating a girl that I was beginning to like, and then suddenly she stopped calling me and only after probably too many texts on my part, she finally had the courage to send me the ol’ fashioned breakup text. Woe is me, right? I had to do something, so I looked online for a guided climb of Mt. Whitney (the tallest peak in the lower 48, oh yeah!). There didn’t seem to be one going around Labor Day when I was going to have a chance to take a trip, so I shot some guiding companies some emails. I contacted SWS Mountain Guides to see if I could get on a waitlist or if they had a guided hike going on around Labor Day (check them out here, they also do many other guided climbs and I think I’m going to do Shasta 2013 Summer - they were great, so I do recommend them). One of the owners responded and said that they were putting a group together for around that time. Next thing I know, he calls me, I give my credit card, and I’m signed up. “Shit!,” I thought, “now I’m going to Mt. Whitney, and I’ve never climbed a mountain, and really haven’t done too much backpacking. I’m just a guy from the suburbs in Columbus, Ohio. How is this going to go?” Fortunately, I had been doing some training for an upcoming half-marathon, so I wasn’t completely out of shape. But, still only 5 weeks to get ready. I stepped up my running and started backpacking with 30 lbs at the various parks around Columbus. I started to get a sharp pain in the ball of my foot from probably over-training. I suspect either Sesamoiditis or Metatarsalgia, but the podiatrist didn’t give me a diagnosis. Had an X-ray to eliminate whether or not it was a stress fracture and would it derail my planned trip (it wasn’t). Which would have been a real bummer, as I was now looking forward to it quite a bit. The doctor made up a custom little orthotic and I ended up just taking it a little easier and the pain began to subside. So, I was good for the trip. Lone Pine and the Portal Flew out to LAX, rented a car, and drove out to Lone Pine. I stayed in the Whitney Portal Hostel for the night (if you do go out there, it’s a nice, cheap place to stay on your way up or out). I think I was the last one in for the night. Everyone in the room kept to themselves and I struck up a short conversation with what seemed to be a drifter type character. He seemed nice, but told stories that had more of a tall tale feel. Wasn’t sure about him, so I spooned with my bag. Lone Pine is a tourist town, the main strip adorned with gift shops, restaurants, and bars. I’m not sure many people actually live in the town. I woke up early the next morning and walked across the street to the Mt. Whitney Restaurant for breakfast. It was good, standard breakfast fare. I decided to do some driving around and see the area. So, I drove out to the Alabama Hills, which are right there. A lot of westerns were filmed in the area. The Alabama Hills definitely have a distinct look. They are these beautiful, reddish-brown, sandy rocks jutting out of the desert, grouped and piled up together. The rest of the day, I spent mostly driving around and taking photographs of the scenery. I drove up to the Whitney Portal, checked it out. Drove down, back to town. Drove out to the main ranger station, picked up a map of the hiking routes and some post cards. Ate lunch in town at the local coffeeshop. I drove over to the Cottonwood Lakes area, which was actually a scarier drive and I think it may have been somewhere around 10,000 feet. That drive seemed to take a while, both up and down. Part of my reasoning for heading up there, along with checking out the area and having nothing else to really do, was to hopefully give my body some chance to acclimate to the higher elevation, given that I have lived my entire life at about 900 feet. In the late afternoon, I finally headed up to the Whitney Portal. I had planned on camping out at the portal in order to give myself a chance to acclimate. Individual campgrounds are right there, just off the parking lot and I think they’re about $5 or $10 for the night. There was really no one else around, so I had my pick of camping spots. So, I set up my tent and everything and took it easy for a little bit. Now, one thing I didn’t realize that I should have done (and I’m claiming ignorance here) is that your tent should be tied down to rocks. This is due to, what I believe, the fact that the ground is mostly rocky, not hardened dirt, and it can also be quite windy at the higher altitudes. There was a pile of rocks near the site and I looked at them somewhat bewildered, thinking that perhaps you put them on or in your tent to hold it down. I put a few rocks in the tent and on the corners. But, I think what you want to do is you want to tie some guy lines from your tent to the rocks to hold them down. So, anyways, I headed off to dinner at the Portal Store. The Portal Store is right at the start of the Whitney Trail. It’s a little shop for last minute supplies and it’s got a kitchen that serves gigantic burgers for lunch. Although I didn’t have breakfast, I believe they also make these huge, plate-sized pancakes. It’s a great stop after a long day of hiking or in my case of driving the twisting roads of the Sierras. So, I had a giant burger and fries. I tried to sit down, but was chased away by all the bees that were after my ketchup and I ended up inhaling my food in fear of an impending bee attack and got out of there. I took it easy the rest of the evening and the weather was beautiful. I sat and read a book by a babbling brook near the campsite. It was great. Day One - To Basecamp Woke up early, packed up my tent, threw everything into my rented Kia Soul and headed back down to Lone Pine to meet the group and the guides. We met outside of the local outdoor gear shop right in the center of town called Elevation Sierra Essentials. It’s owned by a fellow named John, who ended up being one of our guides, and is a truly kind, knowledgeable person. If you end up needing any gear before a Whitney hike, this is the place to stop into. We did a gear check with the guides, Tim and John, making sure we had everything we needed and if not we could just buy it next door. The group consisted of six hikers plus two guides. There was a father and son (Gabe and Michael), two old friends (John and Joann), a single hiker (Esther), and myself. So, we repacked our stuff, found parking for our cars, and we drove up to the portal to begin hiking. We started on the Whitney trail for only a little bit until the the Mountaineer’s Route breaks off. What makes it a route and not a trail is that it isn’t explicitly marked. You can take anyway to get there, it’s more of a guidance. So, once you break from the Whitney Trail, it almost immediately becomes more steep and some bouldering is involved. At this point, one of the hikers, John (who I believe may have had a bad knee) decided to head back. He was out for the trip. The hiking early on isn’t too bad, over some creeks and rocks, but as to be expected from a mountain climb, it is consistently steep. So, one technique that you want to utilize as much as possible is the mountaineer’s rest step. It’s essential for mountaineering. Basically: 1) you take one step forward, allowing your back leg knee to lock, keeping the weight on the back knee; 2) you then shift your weight to the front leg and step up with the back leg, keeping weight off the stepping leg, then locking the knee of the rear leg (back to step one). This allows you to take tiny rests as you are climbing. It also slows you down a little bit and as things get steeper, the amount of steps taken may slow down. There was some minor bouldering and climbing over rocks, but nothing of note until you come upon the Ebersbacher Ledges. Ebersbacher Ledges The Ebersbacher Ledges consist of a quick traverse that is exposed and then climbing of a series of a few accessible ledges until you are clear. The difficulty with the ledges is primarily that of exposure. It’s not serious climbing, it’s simply that a serious loss of balance would end badly. I also think part of it is a bit of a self-fulfilling prophecy. Everyone says they’re tricky and frightening, so they become that way. So, they’re not difficult to climb in a technical, they’re just difficult in the face of exposure. The key, obviously, is steadiness. One thing to note is that, you do need to know where to go up, as from some stories John was telling me, there have been hikers who have continued across the face and realized that they had passed the ledges to get up and then had gotten either stuck in a bad place or had to backtrack. So, getting the Ebersbacher Ledges, we then proceeded across the traverse one-by-one and then up the ledges. One of the hikers got a little spooked, and I felt the same way, so one of the guides coaxed each of us across the traverse to the ledges. It was then up the ledges, hike across a tree, and then up a few more and we were done. We then continued to hike up to Lower Boy Scout Lake. Unfortunately during the hike it began raining, so on came the rain coats and pants, and with it still being quite warm, it was a bit uncomfortable. We got to Lower Boy Scout Lake for lunch and fortunately found a bit of cover to sit and relax as it had begun raining much harder at this point. Lower Boy Scout Lake lies at about 10,300 feet and is still mostly within the tree line. It’s a beautiful spot to sit and relax after a morning’s vigorous hike. I downed a couple of Clif bars (which by the end of the hike I was thoroughly tired of eating. It’s sort of like Champagne, if you have too much you’re going to swear it off for good. That’s me and the peanut butter flavored Clif bars, I just had too many that week) and off we went. Up the mountain, over some water run offs, it can get a little slick, so footing continues to be key and a good pair of hiking poles are nice to have. We climbed another 1,000 feet or so, to our base camp at Upper Boy Scout Lake (~11,300 feet). A tent had already been setup prior to our arrival, most likely from a previous guided climb, and staked out a great spot on the rocks. So, we set up two more tents, dried our stuff off a bit and made a few phone calls (yes, there is cell phone reception up there, wow!?!). Upper Boy Scout Lake is a good spot to camp on this hike, although if you do want to go further, you can make it to Iceberg Lake in time if your pace is good. John then took us out and gave us a tutorial on rope climbing and belaying basics with a group of climbers. We came back, ate some noodles for dinner, and went to bed as the plan was to get up early (2 or 3 am) for the hike to the summit. The sky was clear that night, so I took some time to lay out on a rock and stare up at the stars. Wonderful. To setup a tent where there is no good clay to hold stakes, the best way is to guy line the tent to heavy rocks. It’s also quite windy on Mt. Whitney, so it’s just a good idea. Here’s a quick tutorial on how to tie a taut line: http://youtu.be/ihL5cnj5nKU Day Two - Summit We awoke at 3:30am, a little later than we wanted, tossed on our headlamps, threw on our packs, and headed up the trail to Iceberg Lake (~12,600 feet). The night prior, we unloaded most of the unneccessary stuff from our packs for the summit hike. I had asked Tim why it was that we didn’t need the smaller “Summit Packs” and the fact that they wanted us to carry our large packs and the reason he gave, was that the full packs are better at protecting your entire back should you fall back or hit a rock. It was relatively cold in the early morning, so we warmed up quickly. Because of the late start, Tim, the main guide, pushed the pace a bit. It wasn’t too bad, but some of the folks in the group began slowing up. The hike wasn’t too bad, with a bit of rock scrambling, but nothing overly strenuous. Not until we got to Iceberg Lake, did the climb begin to take on any real difficulty. This was where the true climb began. We arrived around 8am, rested and ate, before beginning the final ascent. Joann at this point began not feeling well and decided not to join us in the climb. For the first part of the climb, it was heavy bouldering. My hands and legs got a bit chewed up from the coarse rock, but nothing terrible. Tim, Gabe, and I began to move a bit quicker than Michael and Esther at this point (John was guiding them), so Tim began pushing us at a quicker pace to attempt a summit climb as clouds were seen moving in. After a couple of hours of scrambling, we got to a rest point called “Piss Rock,” for obvious reasons. Took a short breather and then continued heading up. There was very little snow on the mountain, it was mostly gravel, so footing became an issue and moving quickly was apparent. It was definitely exhausting, maintaining such a solid pace. After what I believe was an hour or two more, we reached the “Notch.” It was here that we made the true final climb to the summit of Mt. Whitney of about 500 feet. We again took a brief breather and had something to eat. First, you have to scramble up a brief precipice before getting to a spot where you mostly just climb up rocks. Tim first scrambled up and got some rope into place. Gabe and I then hooked up and scrambled up. Not terrible, we just had to move quickly. At this point, Tim would go on ahead, get the rope fixed for “on belay” and then we would follow. There were a few blind rocks where we had to pull ourselves up, but nothing like rock climbing a face. Tim was trying to getting us up there quickly as a cloud was moving in and he wanted to get us up and off the summit in order to avoid any chance of lightning strikes. Well, we made it. We got a few minutes at the top. I remember the feeling of elation of reaching the summit. What a good feeling to achieve the summit. I wanted to run over to the house and sign the book, but Tim was obviously nervous at this point and wanted to get us down. I don’t blame him as he is charged with responsibility for us, “the clients.” So, down we went. Unfortunately, Esther and Michael didn’t get the chance to go for the summit as Tim called them off given the oncoming clouds. I actually found the climb down from this point to be more difficult than the climb up as getting my footing was at times blind and uncertain. And as they say “49% is getting to the top.” At the scramble part, as I was trying to slowly ease myself down, I fell. Fortunately, I was on rope, so Tim caught me. It could have been quite ugly. Thank you, Tim. From the Notch down, it wasn’t too bad. Footing really was the issue as we began the descent as the rock is quite loose in this section. Another few more hours and we were back at Iceberg Lake, with the most difficult portion of the climb past us. We took a nice break. At this point it was the afternoon, so we started out to Upper Boy Scout Lake to get back for dinner and rest. At this point, I began dragging a bit and not hiking as quickly. Nearer to Upper Boy Scout Lake there is a large amount of quartz rock. Day Three - Back to Lone Pine We woke up early to a beautiful sunrise over the Sierras, ate a nice hot breakfast of oatmeal, and headed down. We didn’t have to pack up the tents as a group was heading up that day to utilize them, which was a nice bonus. The hike down wasn’t too bad, I was just a bit worn out from the previous day, so my technique at times got bad, allowing my front knee to lock up. Not a good idea and an easy way to injure oneself especially with a 30-40 pound pack. The day was great and as we descended, it got progressively warmer. Down at the Ebersbacher Ledges, I had a bit of moment of freak-out and began to get nervous, got myself a little stuck and uncomfortable on them. Made it through them, though, with some coaxing from John (thank god for John). Unfortunately, I think my boots may have been too small and didn’t leave enough toe room, so I got a serious case of toe bang and lost a few toe nails to it. I really should have been wearing about a half size larger to give my toes so more room. Lesson learned. Well, we got down to Portal, took a few photos and headed back to Lone Pine. We all ate a celebratory lunch at the Mt. Whitney Restaurant, enjoying a nice big burger and beer. I was so worn out that I decided to stay the night in Lone Pine before heading to the coast for some surfing and relaxing on the beach. Further Reading on Mt. Whitney Here’s a link also to another great blog regarding hiking the Mountaineer’s Route of Mt. Whitney that I found helpful before I went: http://www.sierradescents.com/climbing/whitney/mountaineers-route.html","categories":[],"tags":[{"name":"Mt. Whitney","slug":"Mt-Whitney","permalink":"http://ralucas.github.io/tags/Mt-Whitney/"},{"name":"Mountaineer's Route","slug":"Mountaineer-s-Route","permalink":"http://ralucas.github.io/tags/Mountaineer-s-Route/"},{"name":"Mountaineering","slug":"Mountaineering","permalink":"http://ralucas.github.io/tags/Mountaineering/"},{"name":"Climbing","slug":"Climbing","permalink":"http://ralucas.github.io/tags/Climbing/"}],"keywords":[]},{"title":"Hello","slug":"Hello","date":"2012-09-24T00:36:49.000Z","updated":"2016-04-27T10:44:08.000Z","comments":true,"path":"2012/09/23/Hello/","link":"","permalink":"http://ralucas.github.io/2012/09/23/Hello/","excerpt":"","text":"Welcome to my Github Blog and Projects Blog. I focus mostly on Node.js and Javascript. Please feel free to message me anytime through my profile @ralucas, via twitter @r_a_lucas, or email richard@richardalucas.com. Cheers! Richard Current Projects: Node-Zillow","categories":[],"tags":[],"keywords":[]}]}