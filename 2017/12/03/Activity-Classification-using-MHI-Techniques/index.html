<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="CS stuff and more...">
    <meta name="keyword"  content="Python, Computer Vision, Node.js, JavaScript, Networking, TCP, Code, Running, HTTP">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
          Activity Classification using MHI Techniques - Richard Lucas&#39; Blog
        
    </title>

    <link rel="canonical" href="http://ralucas.github.io/2017/12/03/Activity-Classification-using-MHI-Techniques/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Richard Lucas&#39; Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    
<!-- Image to hack wechat -->
<!-- <img src="http://ralucas.github.io/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="{{ site.baseurl }}/{% if page.header-img %}{{ page.header-img }}{% else %}{{ site.header-img }}{% endif %}" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/orion-nebula.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                          <a class="tag" href="/tags/Activity-Classification/" title="Activity Classification">Activity Classification</a>
                        
                          <a class="tag" href="/tags/Computer-Vision/" title="Computer Vision">Computer Vision</a>
                        

                    </div>
                    <h1>Activity Classification using MHI Techniques</h1>
                    
                    <span class="meta">
                        Posted by Richard Lucas on
                        Dec 3 2017
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p><em>Activity classification by computers has a rich history and this paper seeks to describe, analyze, and compare some methods used in classifying a subset of actions. The creation of temporal templates via motion-energy images (MEI) and motion history images (MHI) were utilized to analyze and quantify frames of videos. Image moments were taken from the MHIs and a learning model was built. The results obtained are compared and discussed along with other comparative methods. The activities focused on were boxing, handclapping, handwaving, running, jogging, and walking.</em></p>
<h2 id="Interesting-Links"><a href="#Interesting-Links" class="headerlink" title="Interesting Links"></a>Interesting Links</h2><ul>
<li><strong>Multiple Activity Videos:</strong><ul>
<li>Handclapping &amp; Boxing Video <a href="https://youtu.be/dEtfgAYwCg4" target="_blank" rel="external">https://youtu.be/dEtfgAYwCg4</a></li>
<li>Multitude activity Video <a href="https://youtu.be/x_hFKZzc0jw" target="_blank" rel="external">https://youtu.be/x_hFKZzc0jw</a></li>
</ul>
</li>
</ul>
<h2 id="Related-work-in-recently-published-research"><a href="#Related-work-in-recently-published-research" class="headerlink" title="Related work in recently published research"></a>Related work in recently published research</h2><p>There are a number of research publications that are directly related to activity classification, although some of their methods may vary. The most pertinent related work to this project is that of Bobick and Davis [5], in which the idea of using temporal templates to classify activities is discussed. This technique relies primarily on the creation of Motion-energy images and Motion-history images to create a temporal template. Image moments are then extracted from these templates and a distance model is created.</p>
<p>An alternative to temporal templates is suggested in Schuldt, et al [8] in which the use of space-time features are used with a support vector machine classifier. The space-time features are, in essence, points of interest that occur at specific points over time allowing patterns to be developed. They are constructed through the use of image gradients and a Gaussian convolution kernel in which points of interest are located in comparison to a maxima. The use of SVMs in training a model are then taken to achieve fairly good results (and will be compared later herein).</p>
<p>In more recent work, the use of still poses and poselets (body part detection from joint and part positions) [7] to develop an underlying stick figure representation found good results in identifying activities. Additionally, this research took into account the 3D orientation from a trained model. The method achieved results of ~60% correct identification with just the poselet representation and ~65% when the object model was added. Another recent study [3] utilized summary dynamic rgb images and convolution neural network (CNN) to develop a system in which identification was done quickly and reasonably successfully as the researchers were able to get ~89% successful identification.</p>
<h2 id="Methodology-and-implementation-used"><a href="#Methodology-and-implementation-used" class="headerlink" title="Methodology and implementation used"></a>Methodology and implementation used</h2><p>Temporal templates were used to create modeled images over a period of time, traditional image moments [9][4][6][2] were then taken and a model was trained.</p>
<h3 id="Temporal-Templates"><a href="#Temporal-Templates" class="headerlink" title="Temporal Templates"></a>Temporal Templates</h3><p>Two types of temporal templates were created: motion-energy images (or binary images) (MEIs) and motion-history images (MHIs). In creating these images, each video frame was taken through a preprocessing step in order to better extract the purposeful, activity related motion from the background and tangential motions within each frame. This was done b first grayscaling each frame, then running the frame through OpenCV Gaussian blurring and morphing functions.</p>
<p>The MEIs were created by taking a temporal diff from each frame and assigning coordinates with a 0 or 1 given if the temporal difference was greater than a constant <span>$\theta$</span><!-- Has MathJax --> or not (see examples in figure 1). A $\theta$ of <code>14</code> was used throughout the training and testing phases as it was determined to give the best results.</p>
<div style="display:flex;"><br><img align="left" width="160" height="120" src="/resources/A19C0C2A9DEED833992394DAB24C3EE7.png" alt="mei_image_boxing_person05_d3_10.png"><br><img align="right" alt="mei_image_handwaving_person01_d3_12.png" src="/resources/ADC2C3C9010B694FB9C870F47C89C5EF.png" width="160" height="120"><br></div><br><div style="display:flex;"><br><img align="left" width="160" height="120" src="/resources/206EF4CD0F290B8C44E39F800AAF6826.png" alt="mei_image_walking_person04_d3_20.png"><br><img align="right" width="160" height="120" src="/resources/889A6EB6627389DACAA15AFCB8DDBE01.png" alt="mei_image_handclapping_person05_d3_4.png"><br></div><br><div style="display:flex;"><br><img align="left" width="160" height="120" src="/resources/58CCB63298A0D0C74797DC0B6133BE59.png" alt="mei_image_jogging_person04_d3_0.png"><br><img align="right" width="160" height="120" src="/resources/646C7B58F989519A1C2838B9718699DF.png" alt="mei_image_running_person05_d3_2.png"><br></div>

<p><strong>Figure 1</strong>: <em>Clockwise, starting in left corner: Boxing, Handwaving, Handclapping, Running, Jogging, Walking</em></p>
<p>The MHIs were built from those binary images and provide a more nuanced and graded view of the motion over time. These were calculated utilizing hand-crafted constants <span>$\tau$</span><!-- Has MathJax -->, in which over time gradients were developed. Figure 2 shows an example of different people handwaving as it shows more of the change over time versus the constant that the MEIs show.</p>
<p>In creation of these temporal templates, the $\tau$ constant value for each activity was additionally used (Table 1) a demarcation for how many frames to analyze from a video, so the video sequences (as outlined in the sequences[1]) were divided further into small $\tau$ based chunks. This allowed for consistent training of an activity and expanding the total number of training samples, therefore giving more to the trained model to improve upon prediction.</p>
<table>
<thead>
<tr>
<th>Activity</th>
<th>Tau</th>
</tr>
</thead>
<tbody>
<tr>
<td>boxing</td>
<td>7</td>
</tr>
<tr>
<td>handclapping</td>
<td>13</td>
</tr>
<tr>
<td>handwaving</td>
<td>17</td>
</tr>
<tr>
<td>jogging</td>
<td>21</td>
</tr>
<tr>
<td>running</td>
<td>11</td>
</tr>
<tr>
<td>walking</td>
<td>35</td>
</tr>
</tbody>
</table>
<p><em>Table 1</em></p>
<div style="display:flex;"><br><img align="left" width="160" height="120" src="/resources/31DF5A4229040E941925BCAA6766B5A9.png" alt="mhi_image_boxing_person04_d3_3.png"><br><img align="right" width="160" height="120" src="/resources/A5627A0DA267394D6B3BE27EC2A0AB91.png" alt="mhi_image_handwaving_person05_d3_30.png"><br></div><br><div style="display:flex;"><br><img align="left" width="160" height="120" src="/resources/5DAB00468AFD541B0A7E0165381C3E02.png" alt="mhi_image_walking_person05_d3_7.png"><br><img align="right" width="160" height="120" src="/resources/F1E8F01CB2227A18975836934E58F422.png" alt="mhi_image_handclapping_person03_d3_6.png"><br></div><br><div style="display:flex;"><br><img align="left" width="160" height="120" src="/resources/E56FAD24DF1E1D637212375E26C0A87D.png" alt="mhi_image_jogging_person02_d3_7.png"><br><img align="right" width="160" height="120" src="/resources/70539DBC7EF87041DC08B733DFDD0242.png" alt="mhi_image_running_person05_d3_10.png"><br></div>

<p><strong>Figure 2:</strong> <em>Clockwise, starting in left corner: Boxing, Handwaving, Handclapping, Running, Jogging, Walking</em> </p>
<h3 id="Image-Moments"><a href="#Image-Moments" class="headerlink" title="Image Moments"></a>Image Moments</h3><p>Image moments for each MHI were taken to develop a feature set upon which the temporal templates could be compared. The central ($\mu$) and scale invariant ($\nu$) moments were calculated for the following moments $pq \in {20, 11, 02, 30, 21, 12, 03, 22}$ [4]. The 7 main Hu Moments [6][2] were then calculated from the scale invariant moments. However, after much testing and training, it was determined that the scale invariant moments gave the best results.</p>
<h3 id="Training-and-Prediction"><a href="#Training-and-Prediction" class="headerlink" title="Training and Prediction"></a>Training and Prediction</h3><p>A large dataset of videos [1] was used as input for training. The K-nearest neighbor algorithm was used to train, classify, and predict the videos. The K-nearest neighbor algorithm works by classifying input based on a plurality vote of nearest neighbors. The features used for training and prediction for this dataset was found to be a combination of the scale invariant features ($\nu$) and the Hu moments. The primary factor here was, in using the Hu moments as features, that enough differentiation was not provided and there was an increase in false positives when testing.</p>
<h2 id="Results-and-performance-statistics"><a href="#Results-and-performance-statistics" class="headerlink" title="Results and performance statistics"></a>Results and performance statistics</h2><p><em>Figure 3: Training results | 90.8% accuracy</em><br><img width="460" height="345" src="/resources/5EC509D8F11D712DBCE8FF4606A860E2.png" alt="training.png"></p>
<p><em>Figure 4: Test results | 84.1% accuracy</em><br><img width="460" height="345" src="/resources/64911BF7A62AE57DF347A9A11D123452.png" alt="test.png"></p>
<p>Figures 3 and 4 give a confusion matrix on the overall results from training and testing.</p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>In addition to the single activity videos used to train and test against, videos with multiple activities within the same video were run as an experiment (sample frames in Figure 6). The method used for each multiple activity video did the following:</p>
<p><em>Labels = Dictionary of labels $\rightarrow$ $\tau$ values</em> initialize<br><span>$FrameList=1 + max(\tau)$</span></p>
<p>As outlined in Algorithm [dmap], multiple activity prediction, first gets an initial prediction by building an array of frames. Then as that frame list is iterated over and $mei$ and $mhi$ images are built, when the number of frames is equal to a $\tau$ value, a prediction from the predict engine is created based on the current $mhi$ values and that prediction is compared to the current $\tau$ label. If they are equal, then that label is returned and a $\tau$ number of frames is written and classified with that label.</p>
<p><img width="500" src="/resources/algorithm1.png" alt="algorithm1"></p>
<h2 id="Analysis-of-results"><a href="#Analysis-of-results" class="headerlink" title="Analysis of results"></a>Analysis of results</h2><p>The six activities could be further categorized into two primary groups: stationary activities and moving activities. Given this, focus can be placed on results within these categories. The most glaring issue is that of the <em>handclapping</em> results. The trainer most commonly confused it with <em>handwaving</em> and <em>boxing</em>, which does have similar motions.</p>
<p>The moving activities also were difficult to separate, and particularly <em>running</em> and <em>jogging</em> proved to be quite difficult to distinguish between. However, given reasonably differentiated $\tau$ values, the variation was carried through. It could also be argued that the difference between jogging and running may be quite different from person to person, so I still believe that these two classifications are somewhat interchangeable.</p>
<h3 id="Analysis-of-experiment"><a href="#Analysis-of-experiment" class="headerlink" title="Analysis of experiment"></a>Analysis of experiment</h3><p>As shown in Table 2, the performance times of algorithm used in the experiment was fairly slow. It took close to half a second to complete a full run of the algorithm of frame set analysis and write, which entails running the algorithm (see above) and write the newly labeled frames to video.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Time (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Frames (36) analysis and prediction</td>
<td>0.45s</td>
</tr>
<tr>
<td>Moments calculation</td>
<td>0.013s</td>
</tr>
<tr>
<td>Temporal image creation</td>
<td>0.15s</td>
</tr>
</tbody>
</table>
<p><em>Table 2</em></p>
<p>The accuracy of the multiple activity videos was hit-or-miss. For example, in the video of just <em>handclapping</em> and <em>boxing</em>, the accuracy was outstanding, showing an above 90% accuracy (see Figure 5). However, in the multitude activity video, accuracy declined. Some of this was due to the definition of <em>jogging</em> versus <em>running</em>, but there were also misclassifications (see Figure 6). Throughout the multiple activity videos, there were misclassifications (see Figure 11), mostly among their two major groups (as discussed above), i.e. <em>walking</em> getting classified as <em>jogging</em>, <em>boxing</em> as <em>handclapping</em>, etc.</p>
<p><em>Figure 5: Handclapping and Boxing | 90.7% accuracy</em><br><img src="/resources/4E165FF834E416F58336EF37445CE402.png" alt="handclapping_boxing_matrix.png"><br><em>N.B. $nan$ means that label was not expected</em></p>
<p><em>Figure 6: Multitude | 36.4% accuracy</em><br><img src="/resources/2F9E7EA10E826520B4DB2FED7F3AEDAB.png" alt="multitude_matrix.png"><br><em>N.B. $nan$ means that label was not expected</em></p>
<div style="display:flex;"><br><img align="left" width="285" height="160" src="/resources/095BFCDDB5ACD1A9E10E6C8440C2341E.png" alt="experiment_1_160"><br><img align="right" width="285" height="160" src="/resources/5EC4CE79902E1C304DCE17A1B94331B7.png" alt="experiment_3_1232"><br></div><br><div style="display:flex;"><br><img align="left" width="285" height="160" src="/resources/89A45B61265B78D82C15B9CCBC5BD161.png" alt="experiment_2_1096"><br><img align="right" width="285" height="160" src="/resources/F101A2C26F3D379CA8E578F7C3D761C4.png" alt="experiment_2_524"><br></div><br><div style="display:flex;"><br><img align="left" width="285" height="160" src="/resources/A715ADD3B15BCD043840813FC32E721F.png" alt="experiment_3_468"><br><img align="right" width="285" height="160" src="/resources/FB3BA8C32C0C1DFFB7DD20B769DA7645.png" alt="experiment_1_1076"><br></div>

<p><strong>Figure 7:</strong> <em>Clockwise, starting in left corner: Boxing (correctly identified), Handwaving (incorrectly identified as clapping), Boxing (incorrectly identified as waving), Running (incorrectly identified as jogging), Walking (incorrectly identified as jogging), Handwaving (correctly identified</em></p>
<h3 id="Analysis-of-weaknesses"><a href="#Analysis-of-weaknesses" class="headerlink" title="Analysis of weaknesses"></a>Analysis of weaknesses</h3><p>There were some weaknesses that came to light in running these<br>predictions:</p>
<ul>
<li>Currently the method requires differing $\tau$ values or if they are similar, quite different $mhi$ images.</li>
<li>All the $\tau$ values were carefully set and vetted. Simply not a scalable process.</li>
<li>As the discussed above, the performance speed is quite slow.</li>
<li>Still a fairly custom and brittle development process.</li>
</ul>
<h2 id="Comparison-to-the-state-of-the-art-methods"><a href="#Comparison-to-the-state-of-the-art-methods" class="headerlink" title="Comparison to the state-of-the-art methods"></a>Comparison to the state-of-the-art methods</h2><p>In comparison of the methodology used to the state-of-the-art methods, we can look at the results obtained from usage of an identical dataset from the Schuldt, et al [8]. In comparing the confusion matrices from the Local SVM Approach to this one, the results obtained from the usage of MHI were quite better and more consistently accurate.</p>
<p><img src="/resources/1F9C0CCB9196C390E225FB74D38F5D0C.png" alt="svm_comp.png"></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>  This paper</td>
<td>84.1%</td>
</tr>
<tr>
<td>  Bilen et al. [3]</td>
<td>89.1%</td>
</tr>
<tr>
<td>  Zha et al.</td>
<td>89.6%</td>
</tr>
<tr>
<td>  â€¦others</td>
<td>~88.0%</td>
</tr>
</tbody>
</table>
<p><em>Table 3</em></p>
<h2 id="Proposals-for-improvement"><a href="#Proposals-for-improvement" class="headerlink" title="Proposals for improvement"></a>Proposals for improvement</h2><p>There are four proposals for improvement that I believe can improve the performance and success rate of predicting activities:</p>
<ul>
<li>Utilizing hidden markov models as a part of the prediction process, particularly in prediction of multiple activities. This would additionally require more training input.</li>
<li>Using poselets and stick figure representations as used in referenced work [7].</li>
<li>Building and utilizing convolutional neural networks, similar to the referenced work [3].</li>
<li>More and variegated training input videos.</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Action videos. <a href="http://www.nada.kth.se/cvap/actions/" target="_blank" rel="external">http://www.nada.kth.se/cvap/actions/</a>.<br>[2] Hu moments | opencv documentation. <https: docs.opencv.org="" 2.4="" modules="" imgproc="" doc="" structural="" analysis="" and="" shape="" descriptors.html?highlight="cvmatchshapes#humoments">. [Online; accessed 2-December-2017].<br>[3] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, and S. Gould. Dynamic image networks for action recognition. In 2016 <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 3034-3042, June 2016.<br>[4] A. F. Bobick. 8D-L2 activity recognition | CS6476 udacity lectures. <a href="https://classroom.udacity.com/courses/ud810/lessons/3513198914/concepts/34988000940923" target="_blank" rel="external">https://classroom.udacity.com/courses/ud810/lessons/3513198914/concepts/34988000940923</a>.<br>[5] A. F. Bobick and J. W. Davis. The recognition of human movement using temporal templates. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 23(3):257-267, Mar 2001.<br>[6] Ming-Kuei Hu. Visual pattern recognition by moment invariants. <em>IRE Transactions on Information Theory</em>, 8(2):179-187, February 1962.<br>[7] S. Maji, L. Bourdev, and J. Malik. Action recognition from a distributed representation of pose and appearance. In <em>CVPR 2011</em>, pages 3177-3184, June 2011.<br>[8] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: a local svm approach. In <em>Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., volume 3</em>, pages 32-36 Vol.3, Aug 2004.<br>[9] Wikipedia. Image moments | wikipedia, the free encyclopedia. <a href="https://en.wikipedia.org/w/index.php?title=LaTeX&amp;oldid=413720397" target="_blank" rel="external">https://en.wikipedia.org/w/index.php?title=LaTeX&amp;oldid=413720397</a>, 2017. [Online; accessed 2-December-2017].</https:></p>


                <hr>

                

                <ul class="pager">
                    
                    
                        <li class="next">
                            <a href="/2017/01/09/Actor-Model-in-Erlang/" data-toggle="tooltip" data-placement="top" title="Actor Model in Erlang">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                

                

            </div>

    <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
            </div>

        </div>
    </div>
</article>

<section id="comments" class="container">
  <div id="disqus_thread"></div>
<section>

<!-- disqus start -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "ralucas";
    var disqus_identifier = "http://ralucas.github.io/2017/12/03/Activity-Classification-using-MHI-Techniques/";
    var disqus_url = "http://ralucas.github.io/2017/12/03/Activity-Classification-using-MHI-Techniques/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
<!-- disqu(s end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("http://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                    <li>
                        <a target="_blank" href="https://twitter.com/r_a_lucas">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                    <li>
                        <a href="mailto:richard@richardalucas.com">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/ralucas">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Richard Lucas&#39; Blog 2018 
                    <br>
                    Contact at <a href="mailto:richard@richardalucas.com">richard@richardalucas.com</a> 
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://ralucas.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("http://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-50082017-4';
    var _gaDomain = 'blog.richardalucas.com';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->




<!-- Image to hack wechat -->
<img src="http://ralucas.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>
